{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic Pointing Decoder\n",
    "\n",
    "#### Input\n",
    "We have encoded the information from a question-document pair in the coattention encoding matrix U. U is of shape 2 * lenght of the word vectors * words in the document. For the moment, we'll assume U is of shape 600 * 600 since our word vectors have length 300 and the max sequence length is set at 600. In this notebook, we'll start using a dummy U of these dimensions.\n",
    "\n",
    "#### Decoder\n",
    "The decoder iteratively estimates the answer span by alternating between predicting the start and end points. It consists of standard LSTM cells as well as HIghway Maxout Networks. Actually, it's two networks: one that estimates the start point, and one that estimates the end point. The networks are identical in architecture, but do not share parameters. In the big picture, we alternate between these two networks to get our answer span. As the networks are identical, we'll refer to them as the network.\n",
    "\n",
    "#### Maxout Networks\n",
    "[EXPLANATION ON MAXOUT NETWORKS HERE]\n",
    "\n",
    "#### Highway Networks\n",
    "[EXPLANATION ON HIGHWAY NETWORKS HERE]\n",
    "\n",
    "#### Highway Maxout Networks\n",
    "[EXPLANATION ON HIGHWAY MAXOUT NETWORKS HERE]\n",
    "\n",
    "#### Cell inputs\n",
    "The network outputs start and end scores for each word in the document and the final estimate is just the argmax of that list. How these individual scores are computed is the tricky part. The score for one word is the output of a Highway Maxout Network, which takes in the coattention encoding of that word, the hidden state of the model (we'll get back to this), the coattention encoding of the previous start point estimate, and the coattention encoding of the previous end point estimate. \n",
    "\n",
    "The hidden state of the LSTM, which is fed into the HMW, is dependent on the LSTM's previous hidden state, the coattention encoding of the previous start estimate, and the coattention encoding of the previous end estimate. Same as the HMN, except that it doesn't look at a specific word, but just keeps track of the hidden state. \n",
    "\n",
    "It's important to note these inputs (for the LSTM and the HMN) are the same for both (start and end) networks: the network estimating the start position takes into account the previous end point estimate, and vice versa. So they need to somehow communicate.\n",
    "\n",
    "#### HMN in detail\n",
    "Okay, let's have a shot at describing the HMN model in layman's terms.\n",
    "\n",
    "- First, we have a layer that puts together everything we have at the moment: the hidden state and the previous start and end estimates. These values are concatenated and multiplied by a weight matrix. Then we apply a tanh nonlinearity over the product.\n",
    "\n",
    "- The second layer concatenates the coattention encoding of the current word with the output of the tanh in the previous layer, puts it into a linear function (Wx +b) and takes the max over the first dimension of the resulting tensor. \n",
    "\n",
    "- The third layer takes the output of the second layer, puts it in a linearity and again applies a max over the first dimension of the product. Essentially the same as the second layer. \n",
    "\n",
    "- The fourth and last layer concatenates the outputs of both the second and third layer, and then does the same operation. The Highway part of the network just means that you use outputs from not just the layer before but also from earlier layers as input. (I think)\n",
    "\n",
    "- To train the network, we minimize the cumulative softmax cross entropy of the start and end points across all iterations. The iterative procedure halts when both the estimate of the start position and the estimate of the end position no longer change, or when a maximum number of iterations is reached. We set the maximum number of iterations to 4 and use a maxout pool size of 16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define inputs\n",
    "m = 600\n",
    "l = 200\n",
    "\n",
    "U = Variable(torch.rand(2*l, m), requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([400, 600])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-4-1f863f63cbcf>, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-4-1f863f63cbcf>\"\u001b[1;36m, line \u001b[1;32m9\u001b[0m\n\u001b[1;33m    shape list(inputs.size())\u001b[0m\n\u001b[1;37m             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class MaxOut(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, output_size, pool_size):\n",
    "        super().__init__()\n",
    "        self.input_size, self.output_size, self.pool_size = input_size, output_size, pool_size\n",
    "        self.lin = nn.Linear(input_size, output_size * pool_size)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        shape list(inputs.size())\n",
    "        shape[-1] = self.output_size\n",
    "        shape.append(self.pool_size)\n",
    "        max_dim = len(shape) -1\n",
    "        out = self.lin(inputs)\n",
    "        m,i = out.view(*shape).max(max_dim)\n",
    "        return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define HMN\n",
    "\n",
    "class HMN(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, pool_size):\n",
    "        super(HMN, self).__init__()\n",
    "        \n",
    "        self.r_linear = nn.Linear(5*input_size, input_size, bias=False)\n",
    "        self.r_tanh = nn.Tanh()\n",
    "        \n",
    "        self.m1 = MaxOut(3*input_size, input_size, pool_size)\n",
    "        self.m2 = MaxOut(input_size, input_size, pool_size)\n",
    "        self.m3 = MaxOut(2*input_size, 1, pool_size)\n",
    "        \n",
    "    def forward(self, h_i, u_t, u_start_prev, u_end_prev):\n",
    "        \n",
    "        batch_size = u_start_prev.size()[0]\n",
    "        # Reshape hidden state to single vector\n",
    "        h_i = h_i.view(h_i.size()[2])\n",
    "        # Copy hidden state for each sample in batch\n",
    "        h_i = h_i.expand(batch_size, h_i.size()[0])\n",
    "        hi_us_ue = torch.cat((h_i, u_start_prev, u_end_prev), dim=1)\n",
    "        \n",
    "        r = self.r_linear(hi_us_ue)\n",
    "        r = self.R_tanh(r)\n",
    "        \n",
    "        print(u_t.size(), r.size())\n",
    "        # should be 32x400, 32x200\n",
    "        u_r = torch.cat((u_t,r),dim=1)\n",
    "        print(u_r.size())\n",
    "        # should be 32x600\n",
    "        m1 = self.m1(u_r)\n",
    "        m2 = self.m2(m1)\n",
    "        m1_m2 = torch.cat((m1,m2))\n",
    "        m3 = self.m3(m1_m2)\n",
    "        out = np.argmax(m3)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define full decoder netowrk\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, pool_size, num_layers):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.lstm = nn.LSTM(4*input_size, hidden_size, num_layers)\n",
    "        self.hmn = HMN(input_size, pool_size)\n",
    "        \n",
    "    def forward(self, u_t, u_start_prev, u_end_prev):\n",
    "        x = torch.cat((u_start_prev, u_end_prev), dim=1)\n",
    "        x = x.view(32,1, 4*l)\n",
    "        # x should be [batchsize x 4l x 1]\n",
    "        # or [batchsize x 1 x 4l]??\n",
    "        _,(h_t, _) = self.lstm(x)\n",
    "        out = self.hmn(h_t, u_t, u_start_prev, u_end_prev)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MaxOut' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-2ddf31443caa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-12e9b4a3f7f3>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, input_size, hidden_size, pool_size, num_layers)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlstm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhmn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mHMN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpool_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mu_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mu_start_prev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mu_end_prev\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-c48a01ba75c7>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, input_size, pool_size)\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mr_tanh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTanh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mm1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMaxOut\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpool_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mm2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMaxOut\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpool_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mm3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMaxOut\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpool_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'MaxOut' is not defined"
     ]
    }
   ],
   "source": [
    "# model setup\n",
    "\n",
    "num_layer = 2\n",
    "\n",
    "batch_size = 32\n",
    "learning_rate = 0.0007\n",
    "num_epochs = 10\n",
    "\n",
    "model = Decoder(200,200,16,1)\n",
    "\n",
    "model.cuda()\n",
    "lossfun = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define inputs\n",
    "m = 600\n",
    "l = 200\n",
    "\n",
    "# make a single mock batch\n",
    "u_start_init = Variable(torch.rand(32,2*l),requires_grad=True)\n",
    "u_end_init = Variable(torch.rand(32, 2*l), requires_grad=True)\n",
    "\n",
    "U = Variable(torch.rand(32, 2*l, m), requires_grad=True)\n",
    "y = Variable(torch.rand(32, 1), requires_grad=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "u_start_prev = u_start_init\n",
    "u_end_prev = u_end_init\n",
    "u_start_prev = u_start_prev.cuda()\n",
    "u_end_prev = u_end_prev.cuda()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # for i, U in enumerate(train_data):\n",
    "    x = U\n",
    "    x = x.cuda()\n",
    "    \n",
    "    y = y #lol wtf\n",
    "    \n",
    "    output = model (x, u_start_prev, u_end_prev)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    loss = lossfun(output[0],y)\n",
    "    loss.backward()\n",
    "    \n",
    "    u_start_prev = U[output:] # row or column??\n",
    "    u_end_prev = U[output+6:] # hacky way of setting the end point estimate\n",
    "    \n",
    "    optimizer.step()\n",
    "    if (i+1)%100 == 0:\n",
    "        print('Epoch [%d/%d], Step[%d/%d], Loss: %0.4f'\n",
    "        %(epoch+1, num_epochs, i+1, len(data)//batch_size, loss.data[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a max sequence length of 600 during training and a hidden state size of 200 for all recurrent units, maxout layers, and linear layers\n",
    "\n",
    "- Do linear layers have hidden units? Do maxout layers have hidden units??"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
