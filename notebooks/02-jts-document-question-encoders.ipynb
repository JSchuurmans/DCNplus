{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "\n",
    "from src.dataset import SquadDataset\n",
    "from src.preprocessing import Preprocessing\n",
    "\n",
    "# Clear memory\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook summary\n",
    "In this notebook we'll set up the model architectures required for the first encoders. These encode the words in the documents, and the words in the questions. Both questions and documents are initially encoded by an LSTM:\n",
    "\n",
    "$$ d_t = LSTM_{enc}(d_{t−1}, x_t^D) $$\n",
    "\n",
    "resulting in document encoding matrix\n",
    "\n",
    "$$ D = [d1, . . ., d_m, d_∅] \\text{ of } L * (m+1) \\text{ dimensions} $$\n",
    "\n",
    "and \n",
    "\n",
    "$$ q_t = LSTM_{enc}(q_{t−1}, x_t^D) $$\n",
    "\n",
    "resulting in intermediate question encoding matrix\n",
    "\n",
    "$$ Q' = [q_1, . . ., q_n, q_∅] \\text{ of } L * (n+1) \\text{ dimensions} $$\n",
    "\n",
    "to which we then apply a nonlinearity\n",
    "\n",
    "$$ Q = tanh(W^{(Q)}Q_0 + b(Q)) \\text{ of } L * (n+1) \\text{ dimensions}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "glove_file_path = \"../data/glove.840B.300d.txt\"\n",
    "squad_file_path = \"../data/train-v1.1.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentEncoderLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(DocumentEncoderLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.lstm(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do we have a single document encoding matrix for all docuemnts, or an encoding matrix for each document, where L is the length of the transformed word vectors and m+1 is the number of words in the document plus a sentinel vector.\n",
    "\n",
    "The shape of the input of a neural net is always defined on the level of a single example, as the batch size may vary. The above would suggest that we feed the network word vectors for a whole document. We pass each word vector through the same LSTM and we obtain new, encoded vectors (which incorporate some of their surrounding context).\n",
    "\n",
    "This raises another question: how are we training this encoding? It seems we do not have a target to train on and therefore no error signal, at least in this section on its own. Just feeding the vectors through an LSTM with random weights seems a little pointless. It seems more likely that this is learned by going through the whole architecture. Does this mean that in order to test this we need to have the whole thing set up?\n",
    "\n",
    "After we have both encodings D and Q, we calculate affinity matrix L = (D.transpose Q). This makes it unlikely that the encoders are coupled to the whole network, since it is difficult (impossible?) to disentangle the error signal you backpropagate.\n",
    "\n",
    "SOLUTION: encoders are unsupervised, and they try to learn a mapping from x to x, e.g. they approximate the identity function. So we train the LSTM with backprop and pass our input along as targets. Conceptually, we have the word vectors, which encode meaning of single words. We pass these through an LSTM, which learns word context. So as output we get the same word meanings, which somehow also encapsulate word interactions because they have been through the LSTM. Is this correct??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "# Assuming that the LSTM takes one word at a time and the sizes stay the same through the encoder\n",
    "input_size = 300\n",
    "hidden_size = 300\n",
    "output_size = 300\n",
    "num_layers = 2\n",
    "batch_size = 4\n",
    "learning_rate = 0.0007\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup model\n",
    "model = DocumentEncoderLSTM(input_size, hidden_size, num_layers)\n",
    "model.cuda()\n",
    "lossfun = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we're encoding the data we are learning the identity function. This means we use input data x as our target. This is a 3D Tensor, and the go-to loss function CrossEntropyLoss expects a 2D Tensor (usually labels are 1D, for every example, so 2D). Should we flatten our x? On the other hand, as it's not really classes we're predicting, it might be more intuitive to use the MSE or something similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found pickled GloVe file. Loading...\n",
      "Done. 2195875 words loaded!\n"
     ]
    }
   ],
   "source": [
    "# Get data\n",
    "data = SquadDataset(squad_file_path, glove_file_path, target='text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for i, data_batch in enumerate(dataloader):\n",
    "        x = Variable(data_batch['text'].float())\n",
    "        x = x.cuda()\n",
    "        y = x\n",
    "        \n",
    "        output = model(x)\n",
    "        optimizer.zero_grad()\n",
    "        loss = lossfun(output[0],y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1)%100 ==0:\n",
    "            print('Epoch [%d/%d], Step[%d/%d], Loss: %0.4f'\n",
    "                 %(epoch+1, num_epochs, i+1, len(data)//batch_size, loss.data[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
