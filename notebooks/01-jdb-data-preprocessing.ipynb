{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook summary\n",
    "\n",
    "In this notebook, we perform the necessary data preprocessing. This consists of the following components:\n",
    "\n",
    "    - Loading the SQuAD data\n",
    "    - Tokenization with Stanford CoreNLP\n",
    "    - Embeddings with GloVe, pretrained on 840B Common Crawl, fixed\n",
    "\n",
    "In the end, we require a 2 x n array containing the input data and target for each question-answer pair. The target here is [UNRESOLVED, see questions]. The input data for each example consists of a paragraph and a question, and each are encoded in word vectors. This means both are matrices of p x l and q x l, where p is the number of words in the paragraph, l the length of the word embeddings, and q the number of words in the question. Whether these can be concatenated or should be separate items in an array is [UNRESOLVED, see questions]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions to resolve:\n",
    "\n",
    "- Should the target answer in training be text or two indices? Same goes for the model output. \n",
    "    - Assumption: should be text\n",
    "\n",
    "It seems that the model should output start and end indices for the span of the answer, but is evaluated on the words contained in the span. If this is the case, there has to be a step between model output and evaluation, where the two indices are converted to words in the span. \n",
    "\n",
    "- Where should this conversion from indices to words take place?\n",
    "\n",
    "- Can the document and question embedding matrices be concatenated or should they be passed as separate list items?\n",
    "- Why do we need the answer_start flag if we only match the output text with the target text?\n",
    "- Should the target answer texts be tokenized too?\n",
    "- Should we make full words out of tokenized contractions? (e.g. you're -> you, are | you're -> you, 're)\n",
    "    - Assumption: No, we stick with the original tokens and hope that they're in GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from pycorenlp import StanfordCoreNLP\n",
    "\n",
    "nlp = StanfordCoreNLP('http://localhost:9001')\n",
    "# If server is offline, run the command below in Terminal from the stanford CoreNLP folder\n",
    "# java -mx4g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9001 -timeout 15000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "squad = pd.read_json('../data/train-v1.1.json', orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(442, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so we need to extract a couple of things:\n",
    "    - answers, which are just texts (that should be tokenized? [UNRESOLVED, see questions])\n",
    "    - questions, which should be tokenized and embedded\n",
    "    - paragraphs, which should be tokenized and embedded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Stanford CoreNLP Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text, annotator=nlp):\n",
    "    \"\"\"\n",
    "    Calls the Stanford CoreNLP Tokenizer running on a local server, which tokenizes the input text.\n",
    "    \n",
    "    Returns:\n",
    "    Tokenized text\n",
    "    \"\"\"\n",
    "    annotated_text = annotator.annotate(text, properties={'annotators': 'tokenize', \"outputFormat\": \"json\"})\n",
    "    tokenized_text = []\n",
    "    for token in annotated_text['tokens']:\n",
    "        word = token['word']\n",
    "        tokenized_text.append(word)\n",
    "        \n",
    "    return tokenized_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The GloVe word embeddings\n",
    "\n",
    "From the DCN paper:\n",
    "\n",
    "\"We use as GloVe word vectors pretrained\n",
    "on the 840B Common Crawl corpus (Pennington et al., 2014). We limit the vocabulary\n",
    "to words that are present in the Common Crawl corpus and set embeddings for out-of-vocabulary\n",
    "words to zero. Empirically, we found that training the embeddings consistently led to overfitting and\n",
    "subpar performance, and hence only report results with fixed word embeddings.\"\n",
    "\n",
    "\n",
    "When reading in the GloVe vectors, we found that some vectors were the wrong length and contained odd words (such as name@example.com) and values (such as '.'). We don't know whether this is intrinsic to the data or whether we import it wrong. Either way, out of the 2196016 total lines, 29 were of the wrong length. We therefore decided to drop those 29 vectors and set the embeddings for the corresponding words to 0. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_file_path = \"../data/glove.840B.300d.txt\"\n",
    "\n",
    "def load_glove_embeddings(file_path):\n",
    "    \"\"\"\n",
    "    Loads the glove word vectors from a textfile and parses it into a dictionary with words and vectors.\n",
    "    \n",
    "    Returns:\n",
    "    A dictionary of words and corresponding vectors\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Loading Glove Model\")\n",
    "    with open(file_path,'r', encoding=\"utf8\") as f:\n",
    "        embeddings_dict = {}\n",
    "        cnt = 0\n",
    "        for i, line in enumerate(f):\n",
    "            \n",
    "            split_line = line.split()\n",
    "            \n",
    "            # Skip aberrant lines\n",
    "            if not len(split_line) == 301:\n",
    "                continue \n",
    "\n",
    "            word = split_line[0]\n",
    "            embedding = np.array([float(val) for val in split_line[1:]])\n",
    "            embeddings_dict[word] = embedding\n",
    "            \n",
    "        print(\"Done. {} words loaded!\".format(len(embeddings_dict)))\n",
    "    return embeddings_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Glove Model\n",
      "Done. 2195875 words loaded!\n"
     ]
    }
   ],
   "source": [
    "embeddings = load_glove_embeddings(glove_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed(words, embeddings):\n",
    "    \"\"\"\n",
    "    Takes words and returns corresponding GloVe word embeddings. Returns a zero vector if no embedding is found.\n",
    "    \n",
    "    Returns:\n",
    "    List of word vectors\n",
    "    \"\"\"\n",
    "    word_vectors = np.zeros((len(words), 300))\n",
    "    \n",
    "    for i, word in enumerate(words):\n",
    "        # Match word with vector\n",
    "        try:\n",
    "            vector = embeddings[word]\n",
    "        except KeyError:\n",
    "            # Set to zero vector if no match\n",
    "            vector = np.zeros(300)\n",
    "            \n",
    "        word_vectors[i] = vector\n",
    "    \n",
    "    return word_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Add a check to verify that the amount of null vectors is relatively low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess paragraphs, questions, and answers\n",
    "\n",
    "def preprocess(text):\n",
    "    \"\"\"\n",
    "    Tokenizes and applies word embeddings to a text.\n",
    "    \"\"\"\n",
    "    tokenized_text = tokenize(text)\n",
    "    embedded_text = embed(tokenized_text, embeddings)\n",
    "\n",
    "    return embedded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SquadDataset(Dataset):\n",
    "    \"\"\"Stanford Question Answering Dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, json_file, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            json_file (string): Path to the csv file with annotations.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.dataset = pd.read_json(json_file, orient='records')['data']\n",
    "        self.dataset = self.flatten_data(self.dataset)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        \n",
    "        text = item['text']\n",
    "        question = item['question']\n",
    "        answer = item['answer']\n",
    "        \n",
    "        if self.transform:\n",
    "            text = self.transform(text)\n",
    "            question = self.transform(question)\n",
    "            \n",
    "        sample = {'text': text, 'question': question, 'answer': answer}\n",
    "\n",
    "        return sample\n",
    "    \n",
    "    def flatten_data(self, data):\n",
    "        flat_data = []\n",
    "        for article in data:\n",
    "            for paragraph in article['paragraphs']:\n",
    "                for qa in paragraph['qas']:\n",
    "                    flat_data.append({'text': paragraph['context'], \n",
    "                                      'question': qa['question'], \n",
    "                                      'answer': qa['answers'][0]['text']})\n",
    "        return flat_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "squad_dataset = SquadDataset(json_file='../data/train-v1.1.json', transform=preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_paragraphs shape: (87599,)\n",
      "data_questions shape: (87599,)\n"
     ]
    }
   ],
   "source": [
    "dataset = squad['data']\n",
    "training_data = preprocess(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataset\n",
    "with open(\"..\\data\\\\training_data.txt\", \"wb\") as fp:\n",
    "    pickle.dump(training_data, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if it saved correctly\n",
    "with open(\"..\\data\\\\training_data.txt\", \"rb\") as fp:   # Unpickling\n",
    "    tdata = pickle.load(fp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
