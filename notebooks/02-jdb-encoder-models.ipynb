{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook summary\n",
    "\n",
    "In this notebook we'll set up the model architectures required for the first encoders. These encode the words in the documents, and the words in the questions. Both questions and documents are initially encoded by an LSTM:\n",
    "\n",
    "d_t = LSTM_enc(d_t−1, x_t^D)\n",
    "\n",
    "resulting in document encoding matrix\n",
    "\n",
    "D = [d1, . . ., d_m, d∅] of L x (m+1) dimensions\n",
    "\n",
    "and\n",
    "\n",
    "q_t = LSTM_enc(q_t−1, x_t^D)\n",
    "\n",
    "resulting in intermediate question encoding matrix\n",
    "\n",
    "Q' = [q_1, . . ., q_n, q∅] of L x (n+1) dimensions\n",
    "\n",
    "\n",
    "to which we then apply a nonlinearity\n",
    "\n",
    "Q = tanh(W^(Q)Q_0 + b(Q)) of L x (n+1) dimensions\n",
    "\n",
    "\n",
    "Let's start!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "\n",
    "from src.dataset import SquadDataset\n",
    "from src.preprocessing import Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "glove_file_path = \"../data/glove.840B.300d.txt\"\n",
    "squad_file_path = '../data/train-v1.1.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
<<<<<<< HEAD
<<<<<<< HEAD
<<<<<<< Updated upstream
=======
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentEncoderLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(DocumentEncoderLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out = self.lstm(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do we have a single document encoding matrix for all documents, or an encoding matrix for each document? It seems we have one for each document, where L is the length of the transformed word vectors and m+1 is the number of words in the document plus a sentinel vector. \n",
    "\n",
    "The shape of the input of a neural net is always defined on the level of a single example, as the batch size may vary. The above would suggest that we feed the network word vectors for a whole document. We pass each word vector through the same LSTM and we obtain new, encoded vectors (which incorporate some of their surrounding context).\n",
    "\n",
    "This raises another question: how are we training this encoding? It seems we do not have a target to train on and therefore no error signal, at least in this section on its own. Just feeding the vectors through an LSTM with random weights seems a little pointless. It seems more likely that this is learned by going through the whole architecture. Does this mean that in order to test this we need to have the whole thing set up?\n",
    "\n",
    "After we have both encodings D and Q, we calculate affinity matrix L = (D.transpose Q). This makes it unlikely that the encoders are coupled to the whole network, since it is difficult (impossible?) to disentangle the error signal you backpropagate. \n",
    "\n",
    "SOLUTION: encoders are unsupervised, and they try to learn a mapping from x to x, e.g. they approximate the identity function. So we train the LSTM with backprop and pass our input along as targets. Conceptually, we have the word vectors, which encode meaning of single words. We pass these through an LSTM, which learns word context. So as output we get the same word meanings, which somehow also encapsulate word interactions because they have been through the LSTM. Is this correct??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming that the LSTM takes one word at a time and the sizes stay the same through the encoder \n",
    "input_size = 300\n",
    "hidden1 = 300\n",
    "output_size =300\n",
    "\n",
    "num_layers = 2\n",
    "\n",
    "batch_size = 32\n",
    "learning_rate = 0.0007\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instantiate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DocumentEncoderLSTM(input_size=input_size, hidden_size=hidden1, num_layers=num_layers)\n",
    "\n",
    "model.cuda()\n",
    "lossfun = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we're encoding the data we are learning the identity function. This means we use input data x as our target. This is a 3D Tensor, and the go-to loss function CrossEntropyLoss expects a 2D Tensor (usually labels are 1D, for every example, so 2D). Should we flatten our x? On the other hand, as it's not really classes we're predicting, it might be more intuitive to use the MSE or something similar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossfun = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found pickled GloVe file. Loading...\n",
      "Done. 2195875 words loaded!\n",
      "Wall time: 26.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "data = SquadDataset(squad_file_path, glove_file_path, target='text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(data, batch_size=batch_size,\n",
    "                        shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 600, 300])\n",
      "torch.Size([32, 600, 300])\n",
      "torch.Size([32, 600, 300])\n",
      "torch.Size([32, 600, 300])\n",
      "torch.Size([32, 600, 300])\n",
      "torch.Size([32, 600, 300])\n",
      "torch.Size([32, 600, 300])\n",
      "torch.Size([32, 600, 300])\n",
      "torch.Size([32, 600, 300])\n",
      "torch.Size([32, 600, 300])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\envs\\deep-learning-env\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    323\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 325\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    326\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\envs\\deep-learning-env\\lib\\site-packages\\torch\\nn\\modules\\loss.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    327\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m         \u001b[0m_assert_no_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 329\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize_average\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    330\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    331\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, data_batch in enumerate(dataloader):\n",
    "        x = data_batch['text'].float()\n",
    "        x = Variable(data_batch['text'].float())\n",
    "        print(x.size())\n",
    "        x = x.cuda()\n",
    "        y = x\n",
    "\n",
    "        output = model(x)\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = lossfun(output[0], y)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        if (i+1)%100 == 0:\n",
    "            print('Epoch [%d/%d], Step[%d/%d], Loss: %0.4f'\n",
    "            %(epoch+1, num_epochs, i+1, len(data)//batch_size, loss.data[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "def save_checkpoint(state, is_best, filename='checkpoint_q.pth.tar'):\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, 'model_q_best.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DocumentEncoderLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(DocumentEncoderLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out = self.lstm(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do we have a single document encoding matrix for all documents, or an encoding matrix for each document? It seems we have one for each document, where L is the length of the transformed word vectors and m+1 is the number of words in the document plus a sentinel vector. \n",
    "\n",
    "The shape of the input of a neural net is always defined on the level of a single example, as the batch size may vary. The above would suggest that we feed the network word vectors for a whole document. We pass each word vector through the same LSTM and we obtain new, encoded vectors (which incorporate some of their surrounding context).\n",
    "\n",
    "This raises another question: how are we training this encoding? It seems we do not have a target to train on and therefore no error signal, at least in this section on its own. Just feeding the vectors through an LSTM with random weights seems a little pointless. It seems more likely that this is learned by going through the whole architecture. Does this mean that in order to test this we need to have the whole thing set up?\n",
    "\n",
    "After we have both encodings D and Q, we calculate affinity matrix L = (D.transpose Q). This makes it unlikely that the encoders are coupled to the whole network, since it is difficult (impossible?) to disentangle the error signal you backpropagate. \n",
    "\n",
    "SOLUTION: encoders are unsupervised, and they try to learn a mapping from x to x, e.g. they approximate the identity function. So we train the LSTM with backprop and pass our input along as targets. Conceptually, we have the word vectors, which encode meaning of single words. We pass these through an LSTM, which learns word context. So as output we get the same word meanings, which somehow also encapsulate word interactions because they have been through the LSTM. Is this correct??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming that the LSTM takes one word at a time and the sizes stay the same through the encoder \n",
    "input_size = 300\n",
    "hidden1 = 300\n",
    "output_size =300\n",
    "\n",
    "num_layers = 2\n",
    "\n",
    "batch_size = 32\n",
    "learning_rate = 0.0007\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instantiate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DocumentEncoderLSTM(input_size=input_size, hidden_size=hidden1, num_layers=num_layers)\n",
    "\n",
    "model.cuda()\n",
    "lossfun = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we're encoding the data we are learning the identity function. This means we use input data x as our target. This is a 3D Tensor, and the go-to loss function CrossEntropyLoss expects a 2D Tensor (usually labels are 1D, for every example, so 2D). Should we flatten our x? On the other hand, as it's not really classes we're predicting, it might be more intuitive to use the MSE or something similar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossfun = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found pickled GloVe file. Loading...\n",
      "Done. 2195875 words loaded!\n",
      "Wall time: 9.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "data = SquadDataset(squad_file_path, glove_file_path, target='text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(data, batch_size=batch_size,\n",
    "                        shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Step[100/2737], Loss: 0.0031\n",
      "Epoch [1/30], Step[200/2737], Loss: 0.0026\n",
      "Epoch [1/30], Step[300/2737], Loss: 0.0025\n",
      "Epoch [1/30], Step[400/2737], Loss: 0.0030\n",
      "Epoch [1/30], Step[500/2737], Loss: 0.0027\n",
      "Epoch [1/30], Step[600/2737], Loss: 0.0025\n",
      "Epoch [1/30], Step[700/2737], Loss: 0.0026\n",
      "Epoch [1/30], Step[800/2737], Loss: 0.0023\n",
      "Epoch [1/30], Step[900/2737], Loss: 0.0028\n",
      "Epoch [1/30], Step[1000/2737], Loss: 0.0027\n",
      "Epoch [1/30], Step[1100/2737], Loss: 0.0030\n",
      "Epoch [1/30], Step[1200/2737], Loss: 0.0026\n",
      "Epoch [1/30], Step[1300/2737], Loss: 0.0029\n",
      "Epoch [1/30], Step[1400/2737], Loss: 0.0026\n",
      "Epoch [1/30], Step[1500/2737], Loss: 0.0026\n",
      "Epoch [1/30], Step[1600/2737], Loss: 0.0027\n",
      "Epoch [1/30], Step[1700/2737], Loss: 0.0027\n",
      "Epoch [1/30], Step[1800/2737], Loss: 0.0023\n",
      "Epoch [1/30], Step[1900/2737], Loss: 0.0026\n",
      "Epoch [1/30], Step[2000/2737], Loss: 0.0031\n",
      "Epoch [1/30], Step[2100/2737], Loss: 0.0026\n",
      "Epoch [1/30], Step[2200/2737], Loss: 0.0026\n",
      "Epoch [1/30], Step[2300/2737], Loss: 0.0027\n",
      "Epoch [1/30], Step[2400/2737], Loss: 0.0027\n",
      "Epoch [1/30], Step[2500/2737], Loss: 0.0026\n",
      "Epoch [1/30], Step[2600/2737], Loss: 0.0027\n",
      "Epoch [1/30], Step[2700/2737], Loss: 0.0026\n",
      "Epoch [2/30], Step[100/2737], Loss: 0.0026\n",
      "Epoch [2/30], Step[200/2737], Loss: 0.0026\n",
      "Epoch [2/30], Step[300/2737], Loss: 0.0026\n",
      "Epoch [2/30], Step[400/2737], Loss: 0.0026\n",
      "Epoch [2/30], Step[500/2737], Loss: 0.0030\n",
      "Epoch [2/30], Step[600/2737], Loss: 0.0028\n",
      "Epoch [2/30], Step[700/2737], Loss: 0.0026\n",
      "Epoch [2/30], Step[800/2737], Loss: 0.0025\n",
      "Epoch [2/30], Step[900/2737], Loss: 0.0026\n",
      "Epoch [2/30], Step[1000/2737], Loss: 0.0023\n",
      "Epoch [2/30], Step[1100/2737], Loss: 0.0024\n",
      "Epoch [2/30], Step[1200/2737], Loss: 0.0021\n",
      "Epoch [2/30], Step[1300/2737], Loss: 0.0028\n",
      "Epoch [2/30], Step[1400/2737], Loss: 0.0026\n",
      "Epoch [2/30], Step[1500/2737], Loss: 0.0026\n",
      "Epoch [2/30], Step[1600/2737], Loss: 0.0025\n",
      "Epoch [2/30], Step[1700/2737], Loss: 0.0026\n",
      "Epoch [2/30], Step[1800/2737], Loss: 0.0025\n",
      "Epoch [2/30], Step[1900/2737], Loss: 0.0025\n",
      "Epoch [2/30], Step[2000/2737], Loss: 0.0028\n",
      "Epoch [2/30], Step[2100/2737], Loss: 0.0026\n",
      "Epoch [2/30], Step[2200/2737], Loss: 0.0024\n",
      "Epoch [2/30], Step[2300/2737], Loss: 0.0026\n",
      "Epoch [2/30], Step[2400/2737], Loss: 0.0024\n",
      "Epoch [2/30], Step[2500/2737], Loss: 0.0025\n",
      "Epoch [2/30], Step[2600/2737], Loss: 0.0026\n",
      "Epoch [2/30], Step[2700/2737], Loss: 0.0025\n",
      "Epoch [3/30], Step[100/2737], Loss: 0.0029\n",
      "Epoch [3/30], Step[200/2737], Loss: 0.0022\n",
      "Epoch [3/30], Step[300/2737], Loss: 0.0029\n",
      "Epoch [3/30], Step[400/2737], Loss: 0.0025\n",
      "Epoch [3/30], Step[500/2737], Loss: 0.0026\n",
      "Epoch [3/30], Step[600/2737], Loss: 0.0024\n",
      "Epoch [3/30], Step[700/2737], Loss: 0.0026\n",
      "Epoch [3/30], Step[800/2737], Loss: 0.0025\n",
      "Epoch [3/30], Step[900/2737], Loss: 0.0024\n",
      "Epoch [3/30], Step[1000/2737], Loss: 0.0024\n",
      "Epoch [3/30], Step[1100/2737], Loss: 0.0025\n",
      "Epoch [3/30], Step[1200/2737], Loss: 0.0023\n",
      "Epoch [3/30], Step[1300/2737], Loss: 0.0027\n",
      "Epoch [3/30], Step[1400/2737], Loss: 0.0025\n",
      "Epoch [3/30], Step[1500/2737], Loss: 0.0024\n",
      "Epoch [3/30], Step[1600/2737], Loss: 0.0025\n",
      "Epoch [3/30], Step[1700/2737], Loss: 0.0030\n",
      "Epoch [3/30], Step[1800/2737], Loss: 0.0024\n",
      "Epoch [3/30], Step[1900/2737], Loss: 0.0028\n",
      "Epoch [3/30], Step[2000/2737], Loss: 0.0029\n",
      "Epoch [3/30], Step[2100/2737], Loss: 0.0026\n",
      "Epoch [3/30], Step[2200/2737], Loss: 0.0026\n",
      "Epoch [3/30], Step[2300/2737], Loss: 0.0026\n",
      "Epoch [3/30], Step[2400/2737], Loss: 0.0029\n",
      "Epoch [3/30], Step[2500/2737], Loss: 0.0026\n",
      "Epoch [3/30], Step[2600/2737], Loss: 0.0026\n",
      "Epoch [3/30], Step[2700/2737], Loss: 0.0021\n",
      "Epoch [4/30], Step[100/2737], Loss: 0.0024\n",
      "Epoch [4/30], Step[200/2737], Loss: 0.0028\n",
      "Epoch [4/30], Step[300/2737], Loss: 0.0025\n",
      "Epoch [4/30], Step[400/2737], Loss: 0.0019\n",
      "Epoch [4/30], Step[500/2737], Loss: 0.0026\n",
      "Epoch [4/30], Step[600/2737], Loss: 0.0027\n",
      "Epoch [4/30], Step[700/2737], Loss: 0.0022\n",
      "Epoch [4/30], Step[800/2737], Loss: 0.0026\n",
      "Epoch [4/30], Step[900/2737], Loss: 0.0029\n",
      "Epoch [4/30], Step[1000/2737], Loss: 0.0024\n",
      "Epoch [4/30], Step[1100/2737], Loss: 0.0025\n",
      "Epoch [4/30], Step[1200/2737], Loss: 0.0026\n",
      "Epoch [4/30], Step[1300/2737], Loss: 0.0024\n",
      "Epoch [4/30], Step[1400/2737], Loss: 0.0023\n",
      "Epoch [4/30], Step[1500/2737], Loss: 0.0021\n",
      "Epoch [4/30], Step[1600/2737], Loss: 0.0029\n",
      "Epoch [4/30], Step[1700/2737], Loss: 0.0025\n",
      "Epoch [4/30], Step[1800/2737], Loss: 0.0027\n",
      "Epoch [4/30], Step[1900/2737], Loss: 0.0024\n",
      "Epoch [4/30], Step[2000/2737], Loss: 0.0024\n",
      "Epoch [4/30], Step[2100/2737], Loss: 0.0024\n",
      "Epoch [4/30], Step[2200/2737], Loss: 0.0023\n",
      "Epoch [4/30], Step[2300/2737], Loss: 0.0022\n",
      "Epoch [4/30], Step[2400/2737], Loss: 0.0024\n",
      "Epoch [4/30], Step[2500/2737], Loss: 0.0025\n",
      "Epoch [4/30], Step[2600/2737], Loss: 0.0023\n",
      "Epoch [4/30], Step[2700/2737], Loss: 0.0020\n",
      "Epoch [5/30], Step[100/2737], Loss: 0.0024\n",
      "Epoch [5/30], Step[200/2737], Loss: 0.0026\n",
      "Epoch [5/30], Step[300/2737], Loss: 0.0028\n",
      "Epoch [5/30], Step[400/2737], Loss: 0.0022\n",
      "Epoch [5/30], Step[500/2737], Loss: 0.0023\n",
      "Epoch [5/30], Step[600/2737], Loss: 0.0022\n",
      "Epoch [5/30], Step[700/2737], Loss: 0.0025\n",
      "Epoch [5/30], Step[800/2737], Loss: 0.0025\n",
      "Epoch [5/30], Step[900/2737], Loss: 0.0028\n",
      "Epoch [5/30], Step[1000/2737], Loss: 0.0027\n",
      "Epoch [5/30], Step[1100/2737], Loss: 0.0025\n",
      "Epoch [5/30], Step[1200/2737], Loss: 0.0028\n",
      "Epoch [5/30], Step[1300/2737], Loss: 0.0030\n",
      "Epoch [5/30], Step[1400/2737], Loss: 0.0032\n",
      "Epoch [5/30], Step[1500/2737], Loss: 0.0026\n",
      "Epoch [5/30], Step[1600/2737], Loss: 0.0025\n",
      "Epoch [5/30], Step[1700/2737], Loss: 0.0026\n",
      "Epoch [5/30], Step[1800/2737], Loss: 0.0024\n",
      "Epoch [5/30], Step[1900/2737], Loss: 0.0026\n",
      "Epoch [5/30], Step[2000/2737], Loss: 0.0026\n",
      "Epoch [5/30], Step[2100/2737], Loss: 0.0027\n",
      "Epoch [5/30], Step[2200/2737], Loss: 0.0025\n",
      "Epoch [5/30], Step[2300/2737], Loss: 0.0021\n",
      "Epoch [5/30], Step[2400/2737], Loss: 0.0026\n",
      "Epoch [5/30], Step[2500/2737], Loss: 0.0027\n",
      "Epoch [5/30], Step[2600/2737], Loss: 0.0025\n",
      "Epoch [5/30], Step[2700/2737], Loss: 0.0025\n",
      "Epoch [6/30], Step[100/2737], Loss: 0.0024\n",
      "Epoch [6/30], Step[200/2737], Loss: 0.0023\n",
      "Epoch [6/30], Step[300/2737], Loss: 0.0027\n",
      "Epoch [6/30], Step[400/2737], Loss: 0.0023\n",
      "Epoch [6/30], Step[500/2737], Loss: 0.0024\n",
      "Epoch [6/30], Step[600/2737], Loss: 0.0025\n",
      "Epoch [6/30], Step[700/2737], Loss: 0.0025\n",
      "Epoch [6/30], Step[800/2737], Loss: 0.0026\n",
      "Epoch [6/30], Step[900/2737], Loss: 0.0026\n",
      "Epoch [6/30], Step[1000/2737], Loss: 0.0027\n",
      "Epoch [6/30], Step[1100/2737], Loss: 0.0025\n",
      "Epoch [6/30], Step[1200/2737], Loss: 0.0029\n",
      "Epoch [6/30], Step[1300/2737], Loss: 0.0024\n",
      "Epoch [6/30], Step[1400/2737], Loss: 0.0026\n",
      "Epoch [6/30], Step[1500/2737], Loss: 0.0023\n",
      "Epoch [6/30], Step[1600/2737], Loss: 0.0025\n",
      "Epoch [6/30], Step[1700/2737], Loss: 0.0023\n",
      "Epoch [6/30], Step[1800/2737], Loss: 0.0027\n",
      "Epoch [6/30], Step[1900/2737], Loss: 0.0025\n",
      "Epoch [6/30], Step[2000/2737], Loss: 0.0021\n",
      "Epoch [6/30], Step[2100/2737], Loss: 0.0026\n",
      "Epoch [6/30], Step[2200/2737], Loss: 0.0027\n",
      "Epoch [6/30], Step[2300/2737], Loss: 0.0026\n",
      "Epoch [6/30], Step[2400/2737], Loss: 0.0023\n",
      "Epoch [6/30], Step[2500/2737], Loss: 0.0027\n",
      "Epoch [6/30], Step[2600/2737], Loss: 0.0025\n",
      "Epoch [6/30], Step[2700/2737], Loss: 0.0022\n",
      "Epoch [7/30], Step[100/2737], Loss: 0.0023\n",
      "Epoch [7/30], Step[200/2737], Loss: 0.0026\n",
      "Epoch [7/30], Step[300/2737], Loss: 0.0024\n",
      "Epoch [7/30], Step[400/2737], Loss: 0.0028\n",
      "Epoch [7/30], Step[500/2737], Loss: 0.0025\n",
      "Epoch [7/30], Step[600/2737], Loss: 0.0025\n",
      "Epoch [7/30], Step[700/2737], Loss: 0.0025\n",
      "Epoch [7/30], Step[800/2737], Loss: 0.0022\n",
      "Epoch [7/30], Step[900/2737], Loss: 0.0027\n",
      "Epoch [7/30], Step[1000/2737], Loss: 0.0023\n",
      "Epoch [7/30], Step[1100/2737], Loss: 0.0023\n",
      "Epoch [7/30], Step[1200/2737], Loss: 0.0024\n",
      "Epoch [7/30], Step[1300/2737], Loss: 0.0026\n",
      "Epoch [7/30], Step[1400/2737], Loss: 0.0025\n",
      "Epoch [7/30], Step[1500/2737], Loss: 0.0026\n",
      "Epoch [7/30], Step[1600/2737], Loss: 0.0026\n",
      "Epoch [7/30], Step[1700/2737], Loss: 0.0030\n",
      "Epoch [7/30], Step[1800/2737], Loss: 0.0026\n",
      "Epoch [7/30], Step[1900/2737], Loss: 0.0027\n",
      "Epoch [7/30], Step[2000/2737], Loss: 0.0022\n",
      "Epoch [7/30], Step[2100/2737], Loss: 0.0026\n",
      "Epoch [7/30], Step[2200/2737], Loss: 0.0025\n",
      "Epoch [7/30], Step[2300/2737], Loss: 0.0022\n",
      "Epoch [7/30], Step[2400/2737], Loss: 0.0023\n",
      "Epoch [7/30], Step[2500/2737], Loss: 0.0023\n",
      "Epoch [7/30], Step[2600/2737], Loss: 0.0023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/30], Step[2700/2737], Loss: 0.0023\n",
      "Epoch [8/30], Step[100/2737], Loss: 0.0029\n",
      "Epoch [8/30], Step[200/2737], Loss: 0.0025\n",
      "Epoch [8/30], Step[300/2737], Loss: 0.0024\n",
      "Epoch [8/30], Step[400/2737], Loss: 0.0026\n",
      "Epoch [8/30], Step[500/2737], Loss: 0.0025\n",
      "Epoch [8/30], Step[600/2737], Loss: 0.0028\n",
      "Epoch [8/30], Step[700/2737], Loss: 0.0027\n",
      "Epoch [8/30], Step[800/2737], Loss: 0.0023\n",
      "Epoch [8/30], Step[900/2737], Loss: 0.0026\n",
      "Epoch [8/30], Step[1000/2737], Loss: 0.0025\n",
      "Epoch [8/30], Step[1100/2737], Loss: 0.0025\n",
      "Epoch [8/30], Step[1200/2737], Loss: 0.0028\n",
      "Epoch [8/30], Step[1300/2737], Loss: 0.0028\n",
      "Epoch [8/30], Step[1400/2737], Loss: 0.0027\n",
      "Epoch [8/30], Step[1500/2737], Loss: 0.0028\n",
      "Epoch [8/30], Step[1600/2737], Loss: 0.0028\n",
      "Epoch [8/30], Step[1700/2737], Loss: 0.0023\n",
      "Epoch [8/30], Step[1800/2737], Loss: 0.0024\n",
      "Epoch [8/30], Step[1900/2737], Loss: 0.0028\n",
      "Epoch [8/30], Step[2000/2737], Loss: 0.0027\n",
      "Epoch [8/30], Step[2100/2737], Loss: 0.0027\n",
      "Epoch [8/30], Step[2200/2737], Loss: 0.0027\n",
      "Epoch [8/30], Step[2300/2737], Loss: 0.0025\n",
      "Epoch [8/30], Step[2400/2737], Loss: 0.0025\n",
      "Epoch [8/30], Step[2500/2737], Loss: 0.0023\n",
      "Epoch [8/30], Step[2600/2737], Loss: 0.0024\n",
      "Epoch [8/30], Step[2700/2737], Loss: 0.0022\n",
      "Epoch [9/30], Step[100/2737], Loss: 0.0025\n",
      "Epoch [9/30], Step[200/2737], Loss: 0.0025\n",
      "Epoch [9/30], Step[300/2737], Loss: 0.0024\n",
      "Epoch [9/30], Step[400/2737], Loss: 0.0023\n",
      "Epoch [9/30], Step[500/2737], Loss: 0.0025\n",
      "Epoch [9/30], Step[600/2737], Loss: 0.0022\n",
      "Epoch [9/30], Step[700/2737], Loss: 0.0022\n",
      "Epoch [9/30], Step[800/2737], Loss: 0.0023\n",
      "Epoch [9/30], Step[900/2737], Loss: 0.0022\n",
      "Epoch [9/30], Step[1000/2737], Loss: 0.0023\n",
      "Epoch [9/30], Step[1100/2737], Loss: 0.0027\n",
      "Epoch [9/30], Step[1200/2737], Loss: 0.0027\n",
      "Epoch [9/30], Step[1300/2737], Loss: 0.0026\n",
      "Epoch [9/30], Step[1400/2737], Loss: 0.0025\n",
      "Epoch [9/30], Step[1500/2737], Loss: 0.0025\n",
      "Epoch [9/30], Step[1600/2737], Loss: 0.0026\n",
      "Epoch [9/30], Step[1700/2737], Loss: 0.0025\n",
      "Epoch [9/30], Step[1800/2737], Loss: 0.0028\n",
      "Epoch [9/30], Step[1900/2737], Loss: 0.0025\n",
      "Epoch [9/30], Step[2000/2737], Loss: 0.0027\n",
      "Epoch [9/30], Step[2100/2737], Loss: 0.0027\n",
      "Epoch [9/30], Step[2200/2737], Loss: 0.0022\n",
      "Epoch [9/30], Step[2300/2737], Loss: 0.0023\n",
      "Epoch [9/30], Step[2400/2737], Loss: 0.0023\n",
      "Epoch [9/30], Step[2500/2737], Loss: 0.0024\n",
      "Epoch [9/30], Step[2600/2737], Loss: 0.0024\n",
      "Epoch [9/30], Step[2700/2737], Loss: 0.0023\n",
      "Epoch [10/30], Step[100/2737], Loss: 0.0022\n",
      "Epoch [10/30], Step[200/2737], Loss: 0.0023\n",
      "Epoch [10/30], Step[300/2737], Loss: 0.0024\n",
      "Epoch [10/30], Step[400/2737], Loss: 0.0026\n",
      "Epoch [10/30], Step[500/2737], Loss: 0.0024\n",
      "Epoch [10/30], Step[600/2737], Loss: 0.0026\n",
      "Epoch [10/30], Step[700/2737], Loss: 0.0026\n",
      "Epoch [10/30], Step[800/2737], Loss: 0.0022\n",
      "Epoch [10/30], Step[900/2737], Loss: 0.0023\n",
      "Epoch [10/30], Step[1000/2737], Loss: 0.0022\n",
      "Epoch [10/30], Step[1100/2737], Loss: 0.0024\n",
      "Epoch [10/30], Step[1200/2737], Loss: 0.0022\n",
      "Epoch [10/30], Step[1300/2737], Loss: 0.0025\n",
      "Epoch [10/30], Step[1400/2737], Loss: 0.0025\n",
      "Epoch [10/30], Step[1500/2737], Loss: 0.0027\n",
      "Epoch [10/30], Step[1600/2737], Loss: 0.0025\n",
      "Epoch [10/30], Step[1700/2737], Loss: 0.0022\n",
      "Epoch [10/30], Step[1800/2737], Loss: 0.0025\n",
      "Epoch [10/30], Step[1900/2737], Loss: 0.0027\n",
      "Epoch [10/30], Step[2000/2737], Loss: 0.0025\n",
      "Epoch [10/30], Step[2100/2737], Loss: 0.0023\n",
      "Epoch [10/30], Step[2200/2737], Loss: 0.0024\n",
      "Epoch [10/30], Step[2300/2737], Loss: 0.0024\n",
      "Epoch [10/30], Step[2400/2737], Loss: 0.0027\n",
      "Epoch [10/30], Step[2500/2737], Loss: 0.0026\n",
      "Epoch [10/30], Step[2600/2737], Loss: 0.0029\n",
      "Epoch [10/30], Step[2700/2737], Loss: 0.0026\n",
      "Epoch [11/30], Step[100/2737], Loss: 0.0026\n",
      "Epoch [11/30], Step[200/2737], Loss: 0.0027\n",
      "Epoch [11/30], Step[300/2737], Loss: 0.0025\n",
      "Epoch [11/30], Step[400/2737], Loss: 0.0022\n",
      "Epoch [11/30], Step[500/2737], Loss: 0.0024\n",
      "Epoch [11/30], Step[600/2737], Loss: 0.0026\n",
      "Epoch [11/30], Step[700/2737], Loss: 0.0026\n",
      "Epoch [11/30], Step[800/2737], Loss: 0.0026\n",
      "Epoch [11/30], Step[900/2737], Loss: 0.0027\n",
      "Epoch [11/30], Step[1000/2737], Loss: 0.0027\n",
      "Epoch [11/30], Step[1100/2737], Loss: 0.0027\n",
      "Epoch [11/30], Step[1200/2737], Loss: 0.0022\n",
      "Epoch [11/30], Step[1300/2737], Loss: 0.0022\n",
      "Epoch [11/30], Step[1400/2737], Loss: 0.0024\n",
      "Epoch [11/30], Step[1500/2737], Loss: 0.0024\n",
      "Epoch [11/30], Step[1600/2737], Loss: 0.0026\n",
      "Epoch [11/30], Step[1700/2737], Loss: 0.0025\n",
      "Epoch [11/30], Step[1800/2737], Loss: 0.0022\n",
      "Epoch [11/30], Step[1900/2737], Loss: 0.0023\n",
      "Epoch [11/30], Step[2000/2737], Loss: 0.0023\n",
      "Epoch [11/30], Step[2100/2737], Loss: 0.0026\n",
      "Epoch [11/30], Step[2200/2737], Loss: 0.0022\n",
      "Epoch [11/30], Step[2300/2737], Loss: 0.0024\n",
      "Epoch [11/30], Step[2400/2737], Loss: 0.0024\n",
      "Epoch [11/30], Step[2500/2737], Loss: 0.0025\n",
      "Epoch [11/30], Step[2600/2737], Loss: 0.0025\n",
      "Epoch [11/30], Step[2700/2737], Loss: 0.0027\n",
      "Epoch [12/30], Step[100/2737], Loss: 0.0022\n",
      "Epoch [12/30], Step[200/2737], Loss: 0.0022\n",
      "Epoch [12/30], Step[300/2737], Loss: 0.0024\n",
      "Epoch [12/30], Step[400/2737], Loss: 0.0026\n",
      "Epoch [12/30], Step[500/2737], Loss: 0.0024\n",
      "Epoch [12/30], Step[600/2737], Loss: 0.0024\n",
      "Epoch [12/30], Step[700/2737], Loss: 0.0023\n",
      "Epoch [12/30], Step[800/2737], Loss: 0.0025\n",
      "Epoch [12/30], Step[900/2737], Loss: 0.0022\n",
      "Epoch [12/30], Step[1000/2737], Loss: 0.0024\n",
      "Epoch [12/30], Step[1100/2737], Loss: 0.0027\n",
      "Epoch [12/30], Step[1200/2737], Loss: 0.0025\n",
      "Epoch [12/30], Step[1300/2737], Loss: 0.0021\n",
      "Epoch [12/30], Step[1400/2737], Loss: 0.0024\n",
      "Epoch [12/30], Step[1500/2737], Loss: 0.0024\n",
      "Epoch [12/30], Step[1600/2737], Loss: 0.0028\n",
      "Epoch [12/30], Step[1700/2737], Loss: 0.0025\n",
      "Epoch [12/30], Step[1800/2737], Loss: 0.0022\n",
      "Epoch [12/30], Step[1900/2737], Loss: 0.0021\n",
      "Epoch [12/30], Step[2000/2737], Loss: 0.0026\n",
      "Epoch [12/30], Step[2100/2737], Loss: 0.0023\n",
      "Epoch [12/30], Step[2200/2737], Loss: 0.0024\n",
      "Epoch [12/30], Step[2300/2737], Loss: 0.0026\n",
      "Epoch [12/30], Step[2400/2737], Loss: 0.0025\n",
      "Epoch [12/30], Step[2500/2737], Loss: 0.0025\n",
      "Epoch [12/30], Step[2600/2737], Loss: 0.0028\n",
      "Epoch [12/30], Step[2700/2737], Loss: 0.0025\n",
      "Epoch [13/30], Step[100/2737], Loss: 0.0029\n",
      "Epoch [13/30], Step[200/2737], Loss: 0.0023\n",
      "Epoch [13/30], Step[300/2737], Loss: 0.0023\n",
      "Epoch [13/30], Step[400/2737], Loss: 0.0028\n",
      "Epoch [13/30], Step[500/2737], Loss: 0.0023\n",
      "Epoch [13/30], Step[600/2737], Loss: 0.0022\n",
      "Epoch [13/30], Step[700/2737], Loss: 0.0023\n",
      "Epoch [13/30], Step[800/2737], Loss: 0.0024\n",
      "Epoch [13/30], Step[900/2737], Loss: 0.0023\n",
      "Epoch [13/30], Step[1000/2737], Loss: 0.0025\n",
      "Epoch [13/30], Step[1100/2737], Loss: 0.0024\n",
      "Epoch [13/30], Step[1200/2737], Loss: 0.0025\n",
      "Epoch [13/30], Step[1300/2737], Loss: 0.0024\n",
      "Epoch [13/30], Step[1400/2737], Loss: 0.0022\n",
      "Epoch [13/30], Step[1500/2737], Loss: 0.0027\n",
      "Epoch [13/30], Step[1600/2737], Loss: 0.0025\n",
      "Epoch [13/30], Step[1700/2737], Loss: 0.0024\n",
      "Epoch [13/30], Step[1800/2737], Loss: 0.0025\n",
      "Epoch [13/30], Step[1900/2737], Loss: 0.0026\n",
      "Epoch [13/30], Step[2000/2737], Loss: 0.0023\n",
      "Epoch [13/30], Step[2100/2737], Loss: 0.0026\n",
      "Epoch [13/30], Step[2200/2737], Loss: 0.0026\n",
      "Epoch [13/30], Step[2300/2737], Loss: 0.0023\n",
      "Epoch [13/30], Step[2400/2737], Loss: 0.0023\n",
      "Epoch [13/30], Step[2500/2737], Loss: 0.0028\n",
      "Epoch [13/30], Step[2600/2737], Loss: 0.0025\n",
      "Epoch [13/30], Step[2700/2737], Loss: 0.0024\n",
      "Epoch [14/30], Step[100/2737], Loss: 0.0026\n",
      "Epoch [14/30], Step[200/2737], Loss: 0.0027\n",
      "Epoch [14/30], Step[300/2737], Loss: 0.0024\n",
      "Epoch [14/30], Step[400/2737], Loss: 0.0022\n",
      "Epoch [14/30], Step[500/2737], Loss: 0.0027\n",
      "Epoch [14/30], Step[600/2737], Loss: 0.0025\n",
      "Epoch [14/30], Step[700/2737], Loss: 0.0026\n",
      "Epoch [14/30], Step[800/2737], Loss: 0.0025\n",
      "Epoch [14/30], Step[900/2737], Loss: 0.0020\n",
      "Epoch [14/30], Step[1000/2737], Loss: 0.0024\n",
      "Epoch [14/30], Step[1100/2737], Loss: 0.0026\n",
      "Epoch [14/30], Step[1200/2737], Loss: 0.0023\n",
      "Epoch [14/30], Step[1300/2737], Loss: 0.0028\n",
      "Epoch [14/30], Step[1400/2737], Loss: 0.0024\n",
      "Epoch [14/30], Step[1500/2737], Loss: 0.0027\n",
      "Epoch [14/30], Step[1600/2737], Loss: 0.0027\n",
      "Epoch [14/30], Step[1700/2737], Loss: 0.0022\n",
      "Epoch [14/30], Step[1800/2737], Loss: 0.0027\n",
      "Epoch [14/30], Step[1900/2737], Loss: 0.0024\n",
      "Epoch [14/30], Step[2000/2737], Loss: 0.0026\n",
      "Epoch [14/30], Step[2100/2737], Loss: 0.0023\n",
      "Epoch [14/30], Step[2200/2737], Loss: 0.0025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/30], Step[2300/2737], Loss: 0.0026\n",
      "Epoch [14/30], Step[2400/2737], Loss: 0.0022\n",
      "Epoch [14/30], Step[2500/2737], Loss: 0.0026\n",
      "Epoch [14/30], Step[2600/2737], Loss: 0.0023\n",
      "Epoch [14/30], Step[2700/2737], Loss: 0.0024\n",
      "Epoch [15/30], Step[100/2737], Loss: 0.0026\n",
      "Epoch [15/30], Step[200/2737], Loss: 0.0025\n",
      "Epoch [15/30], Step[300/2737], Loss: 0.0025\n",
      "Epoch [15/30], Step[400/2737], Loss: 0.0026\n",
      "Epoch [15/30], Step[500/2737], Loss: 0.0026\n",
      "Epoch [15/30], Step[600/2737], Loss: 0.0022\n",
      "Epoch [15/30], Step[700/2737], Loss: 0.0024\n",
      "Epoch [15/30], Step[800/2737], Loss: 0.0026\n",
      "Epoch [15/30], Step[900/2737], Loss: 0.0026\n",
      "Epoch [15/30], Step[1000/2737], Loss: 0.0024\n",
      "Epoch [15/30], Step[1100/2737], Loss: 0.0025\n",
      "Epoch [15/30], Step[1200/2737], Loss: 0.0022\n",
      "Epoch [15/30], Step[1300/2737], Loss: 0.0023\n",
      "Epoch [15/30], Step[1400/2737], Loss: 0.0024\n",
      "Epoch [15/30], Step[1500/2737], Loss: 0.0023\n",
      "Epoch [15/30], Step[1600/2737], Loss: 0.0029\n",
      "Epoch [15/30], Step[1700/2737], Loss: 0.0027\n",
      "Epoch [15/30], Step[1800/2737], Loss: 0.0023\n",
      "Epoch [15/30], Step[1900/2737], Loss: 0.0024\n",
      "Epoch [15/30], Step[2000/2737], Loss: 0.0022\n",
      "Epoch [15/30], Step[2100/2737], Loss: 0.0023\n",
      "Epoch [15/30], Step[2200/2737], Loss: 0.0023\n",
      "Epoch [15/30], Step[2300/2737], Loss: 0.0022\n",
      "Epoch [15/30], Step[2400/2737], Loss: 0.0028\n",
      "Epoch [15/30], Step[2500/2737], Loss: 0.0022\n",
      "Epoch [15/30], Step[2600/2737], Loss: 0.0027\n",
      "Epoch [15/30], Step[2700/2737], Loss: 0.0023\n",
      "Epoch [16/30], Step[100/2737], Loss: 0.0023\n",
      "Epoch [16/30], Step[200/2737], Loss: 0.0025\n",
      "Epoch [16/30], Step[300/2737], Loss: 0.0023\n",
      "Epoch [16/30], Step[400/2737], Loss: 0.0024\n",
      "Epoch [16/30], Step[500/2737], Loss: 0.0024\n",
      "Epoch [16/30], Step[600/2737], Loss: 0.0025\n",
      "Epoch [16/30], Step[700/2737], Loss: 0.0023\n",
      "Epoch [16/30], Step[800/2737], Loss: 0.0026\n",
      "Epoch [16/30], Step[900/2737], Loss: 0.0029\n",
      "Epoch [16/30], Step[1000/2737], Loss: 0.0025\n",
      "Epoch [16/30], Step[1100/2737], Loss: 0.0022\n",
      "Epoch [16/30], Step[1200/2737], Loss: 0.0024\n",
      "Epoch [16/30], Step[1300/2737], Loss: 0.0027\n",
      "Epoch [16/30], Step[1400/2737], Loss: 0.0024\n",
      "Epoch [16/30], Step[1500/2737], Loss: 0.0025\n",
      "Epoch [16/30], Step[1600/2737], Loss: 0.0025\n",
      "Epoch [16/30], Step[1700/2737], Loss: 0.0025\n",
      "Epoch [16/30], Step[1800/2737], Loss: 0.0027\n",
      "Epoch [16/30], Step[1900/2737], Loss: 0.0023\n",
      "Epoch [16/30], Step[2000/2737], Loss: 0.0024\n",
      "Epoch [16/30], Step[2100/2737], Loss: 0.0022\n",
      "Epoch [16/30], Step[2200/2737], Loss: 0.0025\n",
      "Epoch [16/30], Step[2300/2737], Loss: 0.0026\n",
      "Epoch [16/30], Step[2400/2737], Loss: 0.0025\n",
      "Epoch [16/30], Step[2500/2737], Loss: 0.0025\n",
      "Epoch [16/30], Step[2600/2737], Loss: 0.0026\n",
      "Epoch [16/30], Step[2700/2737], Loss: 0.0023\n",
      "Epoch [17/30], Step[100/2737], Loss: 0.0024\n",
      "Epoch [17/30], Step[200/2737], Loss: 0.0023\n",
      "Epoch [17/30], Step[300/2737], Loss: 0.0025\n",
      "Epoch [17/30], Step[400/2737], Loss: 0.0024\n",
      "Epoch [17/30], Step[500/2737], Loss: 0.0023\n",
      "Epoch [17/30], Step[600/2737], Loss: 0.0023\n",
      "Epoch [17/30], Step[700/2737], Loss: 0.0025\n",
      "Epoch [17/30], Step[800/2737], Loss: 0.0019\n",
      "Epoch [17/30], Step[900/2737], Loss: 0.0025\n",
      "Epoch [17/30], Step[1000/2737], Loss: 0.0023\n",
      "Epoch [17/30], Step[1100/2737], Loss: 0.0023\n",
      "Epoch [17/30], Step[1200/2737], Loss: 0.0025\n",
      "Epoch [17/30], Step[1300/2737], Loss: 0.0023\n",
      "Epoch [17/30], Step[1400/2737], Loss: 0.0026\n",
      "Epoch [17/30], Step[1500/2737], Loss: 0.0025\n",
      "Epoch [17/30], Step[1600/2737], Loss: 0.0026\n",
      "Epoch [17/30], Step[1700/2737], Loss: 0.0024\n",
      "Epoch [17/30], Step[1800/2737], Loss: 0.0025\n",
      "Epoch [17/30], Step[1900/2737], Loss: 0.0027\n",
      "Epoch [17/30], Step[2000/2737], Loss: 0.0023\n",
      "Epoch [17/30], Step[2100/2737], Loss: 0.0025\n",
      "Epoch [17/30], Step[2200/2737], Loss: 0.0023\n",
      "Epoch [17/30], Step[2300/2737], Loss: 0.0024\n",
      "Epoch [17/30], Step[2400/2737], Loss: 0.0025\n",
      "Epoch [17/30], Step[2500/2737], Loss: 0.0020\n",
      "Epoch [17/30], Step[2600/2737], Loss: 0.0027\n",
      "Epoch [17/30], Step[2700/2737], Loss: 0.0023\n",
      "Epoch [18/30], Step[100/2737], Loss: 0.0022\n",
      "Epoch [18/30], Step[200/2737], Loss: 0.0025\n",
      "Epoch [18/30], Step[300/2737], Loss: 0.0024\n",
      "Epoch [18/30], Step[400/2737], Loss: 0.0022\n",
      "Epoch [18/30], Step[500/2737], Loss: 0.0027\n",
      "Epoch [18/30], Step[600/2737], Loss: 0.0027\n",
      "Epoch [18/30], Step[700/2737], Loss: 0.0021\n",
      "Epoch [18/30], Step[800/2737], Loss: 0.0024\n",
      "Epoch [18/30], Step[900/2737], Loss: 0.0024\n",
      "Epoch [18/30], Step[1000/2737], Loss: 0.0023\n",
      "Epoch [18/30], Step[1100/2737], Loss: 0.0021\n",
      "Epoch [18/30], Step[1200/2737], Loss: 0.0023\n",
      "Epoch [18/30], Step[1300/2737], Loss: 0.0025\n",
      "Epoch [18/30], Step[1400/2737], Loss: 0.0024\n",
      "Epoch [18/30], Step[1500/2737], Loss: 0.0021\n",
      "Epoch [18/30], Step[1600/2737], Loss: 0.0028\n",
      "Epoch [18/30], Step[1700/2737], Loss: 0.0024\n",
      "Epoch [18/30], Step[1800/2737], Loss: 0.0024\n",
      "Epoch [18/30], Step[1900/2737], Loss: 0.0027\n",
      "Epoch [18/30], Step[2000/2737], Loss: 0.0026\n",
      "Epoch [18/30], Step[2100/2737], Loss: 0.0022\n",
      "Epoch [18/30], Step[2200/2737], Loss: 0.0022\n",
      "Epoch [18/30], Step[2300/2737], Loss: 0.0027\n",
      "Epoch [18/30], Step[2400/2737], Loss: 0.0026\n",
      "Epoch [18/30], Step[2500/2737], Loss: 0.0024\n",
      "Epoch [18/30], Step[2600/2737], Loss: 0.0022\n",
      "Epoch [18/30], Step[2700/2737], Loss: 0.0026\n",
      "Epoch [19/30], Step[100/2737], Loss: 0.0023\n",
      "Epoch [19/30], Step[200/2737], Loss: 0.0023\n",
      "Epoch [19/30], Step[300/2737], Loss: 0.0023\n",
      "Epoch [19/30], Step[400/2737], Loss: 0.0027\n",
      "Epoch [19/30], Step[500/2737], Loss: 0.0021\n",
      "Epoch [19/30], Step[600/2737], Loss: 0.0024\n",
      "Epoch [19/30], Step[700/2737], Loss: 0.0025\n",
      "Epoch [19/30], Step[800/2737], Loss: 0.0026\n",
      "Epoch [19/30], Step[900/2737], Loss: 0.0024\n",
      "Epoch [19/30], Step[1000/2737], Loss: 0.0027\n",
      "Epoch [19/30], Step[1100/2737], Loss: 0.0025\n",
      "Epoch [19/30], Step[1200/2737], Loss: 0.0025\n",
      "Epoch [19/30], Step[1300/2737], Loss: 0.0027\n",
      "Epoch [19/30], Step[1400/2737], Loss: 0.0022\n",
      "Epoch [19/30], Step[1500/2737], Loss: 0.0026\n",
      "Epoch [19/30], Step[1600/2737], Loss: 0.0025\n",
      "Epoch [19/30], Step[1700/2737], Loss: 0.0026\n",
      "Epoch [19/30], Step[1800/2737], Loss: 0.0025\n",
      "Epoch [19/30], Step[1900/2737], Loss: 0.0026\n",
      "Epoch [19/30], Step[2000/2737], Loss: 0.0025\n",
      "Epoch [19/30], Step[2100/2737], Loss: 0.0023\n",
      "Epoch [19/30], Step[2200/2737], Loss: 0.0025\n",
      "Epoch [19/30], Step[2300/2737], Loss: 0.0027\n",
      "Epoch [19/30], Step[2400/2737], Loss: 0.0027\n",
      "Epoch [19/30], Step[2500/2737], Loss: 0.0026\n",
      "Epoch [19/30], Step[2600/2737], Loss: 0.0024\n",
      "Epoch [19/30], Step[2700/2737], Loss: 0.0022\n",
      "Epoch [20/30], Step[100/2737], Loss: 0.0024\n",
      "Epoch [20/30], Step[200/2737], Loss: 0.0026\n",
      "Epoch [20/30], Step[300/2737], Loss: 0.0024\n",
      "Epoch [20/30], Step[400/2737], Loss: 0.0023\n",
      "Epoch [20/30], Step[500/2737], Loss: 0.0024\n",
      "Epoch [20/30], Step[600/2737], Loss: 0.0026\n",
      "Epoch [20/30], Step[700/2737], Loss: 0.0021\n",
      "Epoch [20/30], Step[800/2737], Loss: 0.0022\n",
      "Epoch [20/30], Step[900/2737], Loss: 0.0025\n",
      "Epoch [20/30], Step[1000/2737], Loss: 0.0025\n",
      "Epoch [20/30], Step[1100/2737], Loss: 0.0026\n",
      "Epoch [20/30], Step[1200/2737], Loss: 0.0025\n",
      "Epoch [20/30], Step[1300/2737], Loss: 0.0027\n",
      "Epoch [20/30], Step[1400/2737], Loss: 0.0025\n",
      "Epoch [20/30], Step[1500/2737], Loss: 0.0022\n",
      "Epoch [20/30], Step[1600/2737], Loss: 0.0023\n",
      "Epoch [20/30], Step[1700/2737], Loss: 0.0025\n",
      "Epoch [20/30], Step[1800/2737], Loss: 0.0023\n",
      "Epoch [20/30], Step[1900/2737], Loss: 0.0025\n",
      "Epoch [20/30], Step[2000/2737], Loss: 0.0024\n",
      "Epoch [20/30], Step[2100/2737], Loss: 0.0025\n",
      "Epoch [20/30], Step[2200/2737], Loss: 0.0024\n",
      "Epoch [20/30], Step[2300/2737], Loss: 0.0026\n",
      "Epoch [20/30], Step[2400/2737], Loss: 0.0027\n",
      "Epoch [20/30], Step[2500/2737], Loss: 0.0023\n",
      "Epoch [20/30], Step[2600/2737], Loss: 0.0022\n",
      "Epoch [20/30], Step[2700/2737], Loss: 0.0023\n",
      "Epoch [21/30], Step[100/2737], Loss: 0.0023\n",
      "Epoch [21/30], Step[200/2737], Loss: 0.0022\n",
      "Epoch [21/30], Step[300/2737], Loss: 0.0024\n",
      "Epoch [21/30], Step[400/2737], Loss: 0.0021\n",
      "Epoch [21/30], Step[500/2737], Loss: 0.0026\n",
      "Epoch [21/30], Step[600/2737], Loss: 0.0021\n",
      "Epoch [21/30], Step[700/2737], Loss: 0.0026\n",
      "Epoch [21/30], Step[800/2737], Loss: 0.0024\n",
      "Epoch [21/30], Step[900/2737], Loss: 0.0026\n",
      "Epoch [21/30], Step[1000/2737], Loss: 0.0027\n",
      "Epoch [21/30], Step[1100/2737], Loss: 0.0026\n",
      "Epoch [21/30], Step[1200/2737], Loss: 0.0026\n",
      "Epoch [21/30], Step[1300/2737], Loss: 0.0025\n",
      "Epoch [21/30], Step[1400/2737], Loss: 0.0027\n",
      "Epoch [21/30], Step[1500/2737], Loss: 0.0023\n",
      "Epoch [21/30], Step[1600/2737], Loss: 0.0019\n",
      "Epoch [21/30], Step[1700/2737], Loss: 0.0029\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21/30], Step[1800/2737], Loss: 0.0025\n",
      "Epoch [21/30], Step[1900/2737], Loss: 0.0024\n",
      "Epoch [21/30], Step[2000/2737], Loss: 0.0024\n",
      "Epoch [21/30], Step[2100/2737], Loss: 0.0028\n",
      "Epoch [21/30], Step[2200/2737], Loss: 0.0021\n",
      "Epoch [21/30], Step[2300/2737], Loss: 0.0026\n",
      "Epoch [21/30], Step[2400/2737], Loss: 0.0022\n",
      "Epoch [21/30], Step[2500/2737], Loss: 0.0026\n",
      "Epoch [21/30], Step[2600/2737], Loss: 0.0026\n",
      "Epoch [21/30], Step[2700/2737], Loss: 0.0024\n",
      "Epoch [22/30], Step[100/2737], Loss: 0.0024\n",
      "Epoch [22/30], Step[200/2737], Loss: 0.0027\n",
      "Epoch [22/30], Step[300/2737], Loss: 0.0025\n",
      "Epoch [22/30], Step[400/2737], Loss: 0.0026\n",
      "Epoch [22/30], Step[500/2737], Loss: 0.0022\n",
      "Epoch [22/30], Step[600/2737], Loss: 0.0027\n",
      "Epoch [22/30], Step[700/2737], Loss: 0.0024\n",
      "Epoch [22/30], Step[800/2737], Loss: 0.0023\n",
      "Epoch [22/30], Step[900/2737], Loss: 0.0023\n",
      "Epoch [22/30], Step[1000/2737], Loss: 0.0027\n",
      "Epoch [22/30], Step[1100/2737], Loss: 0.0024\n",
      "Epoch [22/30], Step[1200/2737], Loss: 0.0026\n",
      "Epoch [22/30], Step[1300/2737], Loss: 0.0024\n",
      "Epoch [22/30], Step[1400/2737], Loss: 0.0024\n",
      "Epoch [22/30], Step[1500/2737], Loss: 0.0022\n",
      "Epoch [22/30], Step[1600/2737], Loss: 0.0025\n",
      "Epoch [22/30], Step[1700/2737], Loss: 0.0026\n",
      "Epoch [22/30], Step[1800/2737], Loss: 0.0024\n",
      "Epoch [22/30], Step[1900/2737], Loss: 0.0023\n",
      "Epoch [22/30], Step[2000/2737], Loss: 0.0026\n",
      "Epoch [22/30], Step[2100/2737], Loss: 0.0023\n",
      "Epoch [22/30], Step[2200/2737], Loss: 0.0025\n",
      "Epoch [22/30], Step[2300/2737], Loss: 0.0027\n",
      "Epoch [22/30], Step[2400/2737], Loss: 0.0023\n",
      "Epoch [22/30], Step[2500/2737], Loss: 0.0028\n",
      "Epoch [22/30], Step[2600/2737], Loss: 0.0025\n",
      "Epoch [22/30], Step[2700/2737], Loss: 0.0026\n",
      "Epoch [23/30], Step[100/2737], Loss: 0.0023\n",
      "Epoch [23/30], Step[200/2737], Loss: 0.0024\n",
      "Epoch [23/30], Step[300/2737], Loss: 0.0025\n",
      "Epoch [23/30], Step[400/2737], Loss: 0.0023\n",
      "Epoch [23/30], Step[500/2737], Loss: 0.0023\n",
      "Epoch [23/30], Step[600/2737], Loss: 0.0026\n",
      "Epoch [23/30], Step[700/2737], Loss: 0.0022\n",
      "Epoch [23/30], Step[800/2737], Loss: 0.0027\n",
      "Epoch [23/30], Step[900/2737], Loss: 0.0024\n",
      "Epoch [23/30], Step[1000/2737], Loss: 0.0028\n",
      "Epoch [23/30], Step[1100/2737], Loss: 0.0022\n",
      "Epoch [23/30], Step[1200/2737], Loss: 0.0022\n",
      "Epoch [23/30], Step[1300/2737], Loss: 0.0025\n",
      "Epoch [23/30], Step[1400/2737], Loss: 0.0024\n",
      "Epoch [23/30], Step[1500/2737], Loss: 0.0023\n",
      "Epoch [23/30], Step[1600/2737], Loss: 0.0023\n",
      "Epoch [23/30], Step[1700/2737], Loss: 0.0022\n",
      "Epoch [23/30], Step[1800/2737], Loss: 0.0026\n",
      "Epoch [23/30], Step[1900/2737], Loss: 0.0024\n",
      "Epoch [23/30], Step[2000/2737], Loss: 0.0024\n",
      "Epoch [23/30], Step[2100/2737], Loss: 0.0024\n",
      "Epoch [23/30], Step[2200/2737], Loss: 0.0027\n",
      "Epoch [23/30], Step[2300/2737], Loss: 0.0020\n",
      "Epoch [23/30], Step[2400/2737], Loss: 0.0025\n",
      "Epoch [23/30], Step[2500/2737], Loss: 0.0023\n",
      "Epoch [23/30], Step[2600/2737], Loss: 0.0023\n",
      "Epoch [23/30], Step[2700/2737], Loss: 0.0022\n",
      "Epoch [24/30], Step[100/2737], Loss: 0.0026\n",
      "Epoch [24/30], Step[200/2737], Loss: 0.0023\n",
      "Epoch [24/30], Step[300/2737], Loss: 0.0025\n",
      "Epoch [24/30], Step[400/2737], Loss: 0.0027\n",
      "Epoch [24/30], Step[500/2737], Loss: 0.0024\n",
      "Epoch [24/30], Step[600/2737], Loss: 0.0024\n",
      "Epoch [24/30], Step[700/2737], Loss: 0.0027\n",
      "Epoch [24/30], Step[800/2737], Loss: 0.0023\n",
      "Epoch [24/30], Step[900/2737], Loss: 0.0025\n",
      "Epoch [24/30], Step[1000/2737], Loss: 0.0024\n",
      "Epoch [24/30], Step[1100/2737], Loss: 0.0022\n",
      "Epoch [24/30], Step[1200/2737], Loss: 0.0023\n",
      "Epoch [24/30], Step[1300/2737], Loss: 0.0026\n",
      "Epoch [24/30], Step[1400/2737], Loss: 0.0022\n",
      "Epoch [24/30], Step[1500/2737], Loss: 0.0027\n",
      "Epoch [24/30], Step[1600/2737], Loss: 0.0025\n",
      "Epoch [24/30], Step[1700/2737], Loss: 0.0023\n",
      "Epoch [24/30], Step[1800/2737], Loss: 0.0023\n",
      "Epoch [24/30], Step[1900/2737], Loss: 0.0025\n",
      "Epoch [24/30], Step[2000/2737], Loss: 0.0026\n",
      "Epoch [24/30], Step[2100/2737], Loss: 0.0025\n",
      "Epoch [24/30], Step[2200/2737], Loss: 0.0028\n",
      "Epoch [24/30], Step[2300/2737], Loss: 0.0026\n",
      "Epoch [24/30], Step[2400/2737], Loss: 0.0021\n",
      "Epoch [24/30], Step[2500/2737], Loss: 0.0022\n",
      "Epoch [24/30], Step[2600/2737], Loss: 0.0022\n",
      "Epoch [24/30], Step[2700/2737], Loss: 0.0026\n",
      "Epoch [25/30], Step[100/2737], Loss: 0.0021\n",
      "Epoch [25/30], Step[200/2737], Loss: 0.0022\n",
      "Epoch [25/30], Step[300/2737], Loss: 0.0026\n",
      "Epoch [25/30], Step[400/2737], Loss: 0.0023\n",
      "Epoch [25/30], Step[500/2737], Loss: 0.0027\n",
      "Epoch [25/30], Step[600/2737], Loss: 0.0024\n",
      "Epoch [25/30], Step[700/2737], Loss: 0.0024\n",
      "Epoch [25/30], Step[800/2737], Loss: 0.0025\n",
      "Epoch [25/30], Step[900/2737], Loss: 0.0023\n",
      "Epoch [25/30], Step[1000/2737], Loss: 0.0026\n",
      "Epoch [25/30], Step[1100/2737], Loss: 0.0024\n",
      "Epoch [25/30], Step[1200/2737], Loss: 0.0024\n",
      "Epoch [25/30], Step[1300/2737], Loss: 0.0025\n",
      "Epoch [25/30], Step[1400/2737], Loss: 0.0026\n",
      "Epoch [25/30], Step[1500/2737], Loss: 0.0022\n",
      "Epoch [25/30], Step[1600/2737], Loss: 0.0025\n",
      "Epoch [25/30], Step[1700/2737], Loss: 0.0026\n",
      "Epoch [25/30], Step[1800/2737], Loss: 0.0027\n",
      "Epoch [25/30], Step[1900/2737], Loss: 0.0023\n",
      "Epoch [25/30], Step[2000/2737], Loss: 0.0023\n",
      "Epoch [25/30], Step[2100/2737], Loss: 0.0024\n",
      "Epoch [25/30], Step[2200/2737], Loss: 0.0026\n",
      "Epoch [25/30], Step[2300/2737], Loss: 0.0024\n",
      "Epoch [25/30], Step[2400/2737], Loss: 0.0023\n",
      "Epoch [25/30], Step[2500/2737], Loss: 0.0022\n",
      "Epoch [25/30], Step[2600/2737], Loss: 0.0023\n",
      "Epoch [25/30], Step[2700/2737], Loss: 0.0022\n",
      "Epoch [26/30], Step[100/2737], Loss: 0.0023\n",
      "Epoch [26/30], Step[200/2737], Loss: 0.0024\n",
      "Epoch [26/30], Step[300/2737], Loss: 0.0020\n",
      "Epoch [26/30], Step[400/2737], Loss: 0.0024\n",
      "Epoch [26/30], Step[500/2737], Loss: 0.0025\n",
      "Epoch [26/30], Step[600/2737], Loss: 0.0020\n",
      "Epoch [26/30], Step[700/2737], Loss: 0.0022\n",
      "Epoch [26/30], Step[800/2737], Loss: 0.0027\n",
      "Epoch [26/30], Step[900/2737], Loss: 0.0030\n",
      "Epoch [26/30], Step[1000/2737], Loss: 0.0024\n",
      "Epoch [26/30], Step[1100/2737], Loss: 0.0025\n",
      "Epoch [26/30], Step[1200/2737], Loss: 0.0025\n",
      "Epoch [26/30], Step[1300/2737], Loss: 0.0025\n",
      "Epoch [26/30], Step[1400/2737], Loss: 0.0022\n",
      "Epoch [26/30], Step[1500/2737], Loss: 0.0022\n",
      "Epoch [26/30], Step[1600/2737], Loss: 0.0021\n",
      "Epoch [26/30], Step[1700/2737], Loss: 0.0027\n",
      "Epoch [26/30], Step[1800/2737], Loss: 0.0023\n",
      "Epoch [26/30], Step[1900/2737], Loss: 0.0023\n",
      "Epoch [26/30], Step[2000/2737], Loss: 0.0023\n",
      "Epoch [26/30], Step[2100/2737], Loss: 0.0025\n",
      "Epoch [26/30], Step[2200/2737], Loss: 0.0028\n",
      "Epoch [26/30], Step[2300/2737], Loss: 0.0026\n",
      "Epoch [26/30], Step[2400/2737], Loss: 0.0024\n",
      "Epoch [26/30], Step[2500/2737], Loss: 0.0023\n",
      "Epoch [26/30], Step[2600/2737], Loss: 0.0025\n",
      "Epoch [26/30], Step[2700/2737], Loss: 0.0025\n",
      "Epoch [27/30], Step[100/2737], Loss: 0.0025\n",
      "Epoch [27/30], Step[200/2737], Loss: 0.0025\n",
      "Epoch [27/30], Step[300/2737], Loss: 0.0024\n",
      "Epoch [27/30], Step[400/2737], Loss: 0.0027\n",
      "Epoch [27/30], Step[500/2737], Loss: 0.0026\n",
      "Epoch [27/30], Step[600/2737], Loss: 0.0024\n",
      "Epoch [27/30], Step[700/2737], Loss: 0.0022\n",
      "Epoch [27/30], Step[800/2737], Loss: 0.0024\n",
      "Epoch [27/30], Step[900/2737], Loss: 0.0025\n",
      "Epoch [27/30], Step[1000/2737], Loss: 0.0030\n",
      "Epoch [27/30], Step[1100/2737], Loss: 0.0025\n",
      "Epoch [27/30], Step[1200/2737], Loss: 0.0026\n",
      "Epoch [27/30], Step[1300/2737], Loss: 0.0025\n",
      "Epoch [27/30], Step[1400/2737], Loss: 0.0027\n",
      "Epoch [27/30], Step[1500/2737], Loss: 0.0023\n",
      "Epoch [27/30], Step[1600/2737], Loss: 0.0024\n",
      "Epoch [27/30], Step[1700/2737], Loss: 0.0023\n",
      "Epoch [27/30], Step[1800/2737], Loss: 0.0025\n",
      "Epoch [27/30], Step[1900/2737], Loss: 0.0023\n",
      "Epoch [27/30], Step[2000/2737], Loss: 0.0024\n",
      "Epoch [27/30], Step[2100/2737], Loss: 0.0022\n",
      "Epoch [27/30], Step[2200/2737], Loss: 0.0025\n",
      "Epoch [27/30], Step[2300/2737], Loss: 0.0025\n",
      "Epoch [27/30], Step[2400/2737], Loss: 0.0023\n",
      "Epoch [27/30], Step[2500/2737], Loss: 0.0026\n",
      "Epoch [27/30], Step[2600/2737], Loss: 0.0025\n",
      "Epoch [27/30], Step[2700/2737], Loss: 0.0020\n",
      "Epoch [28/30], Step[100/2737], Loss: 0.0022\n",
      "Epoch [28/30], Step[200/2737], Loss: 0.0025\n",
      "Epoch [28/30], Step[300/2737], Loss: 0.0025\n",
      "Epoch [28/30], Step[400/2737], Loss: 0.0022\n",
      "Epoch [28/30], Step[500/2737], Loss: 0.0024\n",
      "Epoch [28/30], Step[600/2737], Loss: 0.0024\n",
      "Epoch [28/30], Step[700/2737], Loss: 0.0023\n",
      "Epoch [28/30], Step[800/2737], Loss: 0.0024\n",
      "Epoch [28/30], Step[900/2737], Loss: 0.0024\n",
      "Epoch [28/30], Step[1000/2737], Loss: 0.0021\n",
      "Epoch [28/30], Step[1100/2737], Loss: 0.0023\n",
      "Epoch [28/30], Step[1200/2737], Loss: 0.0024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [28/30], Step[1300/2737], Loss: 0.0023\n",
      "Epoch [28/30], Step[1400/2737], Loss: 0.0024\n",
      "Epoch [28/30], Step[1500/2737], Loss: 0.0026\n",
      "Epoch [28/30], Step[1600/2737], Loss: 0.0026\n",
      "Epoch [28/30], Step[1700/2737], Loss: 0.0029\n",
      "Epoch [28/30], Step[1800/2737], Loss: 0.0025\n",
      "Epoch [28/30], Step[1900/2737], Loss: 0.0023\n",
      "Epoch [28/30], Step[2000/2737], Loss: 0.0026\n",
      "Epoch [28/30], Step[2100/2737], Loss: 0.0025\n",
      "Epoch [28/30], Step[2200/2737], Loss: 0.0024\n",
      "Epoch [28/30], Step[2300/2737], Loss: 0.0025\n",
      "Epoch [28/30], Step[2400/2737], Loss: 0.0024\n",
      "Epoch [28/30], Step[2500/2737], Loss: 0.0025\n",
      "Epoch [28/30], Step[2600/2737], Loss: 0.0029\n",
      "Epoch [28/30], Step[2700/2737], Loss: 0.0026\n",
      "Epoch [29/30], Step[100/2737], Loss: 0.0024\n",
      "Epoch [29/30], Step[200/2737], Loss: 0.0022\n",
      "Epoch [29/30], Step[300/2737], Loss: 0.0027\n",
      "Epoch [29/30], Step[400/2737], Loss: 0.0022\n",
      "Epoch [29/30], Step[500/2737], Loss: 0.0023\n",
      "Epoch [29/30], Step[600/2737], Loss: 0.0024\n",
      "Epoch [29/30], Step[700/2737], Loss: 0.0025\n",
      "Epoch [29/30], Step[800/2737], Loss: 0.0023\n",
      "Epoch [29/30], Step[900/2737], Loss: 0.0023\n",
      "Epoch [29/30], Step[1000/2737], Loss: 0.0025\n",
      "Epoch [29/30], Step[1100/2737], Loss: 0.0023\n",
      "Epoch [29/30], Step[1200/2737], Loss: 0.0024\n",
      "Epoch [29/30], Step[1300/2737], Loss: 0.0024\n",
      "Epoch [29/30], Step[1400/2737], Loss: 0.0021\n",
      "Epoch [29/30], Step[1500/2737], Loss: 0.0024\n",
      "Epoch [29/30], Step[1600/2737], Loss: 0.0024\n",
      "Epoch [29/30], Step[1700/2737], Loss: 0.0026\n",
      "Epoch [29/30], Step[1800/2737], Loss: 0.0025\n",
      "Epoch [29/30], Step[1900/2737], Loss: 0.0023\n",
      "Epoch [29/30], Step[2000/2737], Loss: 0.0026\n",
      "Epoch [29/30], Step[2100/2737], Loss: 0.0024\n",
      "Epoch [29/30], Step[2200/2737], Loss: 0.0024\n",
      "Epoch [29/30], Step[2300/2737], Loss: 0.0025\n",
      "Epoch [29/30], Step[2400/2737], Loss: 0.0021\n",
      "Epoch [29/30], Step[2500/2737], Loss: 0.0023\n",
      "Epoch [29/30], Step[2600/2737], Loss: 0.0026\n",
      "Epoch [29/30], Step[2700/2737], Loss: 0.0025\n",
      "Epoch [30/30], Step[100/2737], Loss: 0.0027\n",
      "Epoch [30/30], Step[200/2737], Loss: 0.0025\n",
      "Epoch [30/30], Step[300/2737], Loss: 0.0022\n",
      "Epoch [30/30], Step[400/2737], Loss: 0.0028\n",
      "Epoch [30/30], Step[500/2737], Loss: 0.0027\n",
      "Epoch [30/30], Step[600/2737], Loss: 0.0022\n",
      "Epoch [30/30], Step[700/2737], Loss: 0.0026\n",
      "Epoch [30/30], Step[800/2737], Loss: 0.0024\n",
      "Epoch [30/30], Step[900/2737], Loss: 0.0027\n",
      "Epoch [30/30], Step[1000/2737], Loss: 0.0024\n",
      "Epoch [30/30], Step[1100/2737], Loss: 0.0025\n",
      "Epoch [30/30], Step[1200/2737], Loss: 0.0026\n",
      "Epoch [30/30], Step[1300/2737], Loss: 0.0023\n",
      "Epoch [30/30], Step[1400/2737], Loss: 0.0024\n",
      "Epoch [30/30], Step[1500/2737], Loss: 0.0022\n",
      "Epoch [30/30], Step[1600/2737], Loss: 0.0020\n",
      "Epoch [30/30], Step[1700/2737], Loss: 0.0024\n",
      "Epoch [30/30], Step[1800/2737], Loss: 0.0022\n",
      "Epoch [30/30], Step[1900/2737], Loss: 0.0025\n",
      "Epoch [30/30], Step[2000/2737], Loss: 0.0026\n",
      "Epoch [30/30], Step[2100/2737], Loss: 0.0023\n",
      "Epoch [30/30], Step[2200/2737], Loss: 0.0022\n",
      "Epoch [30/30], Step[2300/2737], Loss: 0.0027\n",
      "Epoch [30/30], Step[2400/2737], Loss: 0.0024\n",
      "Epoch [30/30], Step[2500/2737], Loss: 0.0027\n",
      "Epoch [30/30], Step[2600/2737], Loss: 0.0027\n",
      "Epoch [30/30], Step[2700/2737], Loss: 0.0029\n",
      "Wall time: 10h 44min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, data_batch in enumerate(dataloader):\n",
    "        x = data_batch['text'].float()\n",
    "        x = Variable(data_batch['text'].float())\n",
    "        x = x.cuda()\n",
    "        y = x\n",
    "\n",
    "        output = model(x)\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = lossfun(output[0], y)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        if (i+1)%100 == 0:\n",
    "            print('Epoch [%d/%d], Step[%d/%d], Loss: %0.4f'\n",
    "            %(epoch+1, num_epochs, i+1, len(data)//batch_size, loss.data[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "def save_checkpoint(state, is_best, filename='checkpoint_q.pth.tar'):\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, 'model_q_best.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_prec1 = loss.data[0]\n",
    "is_best = True\n",
    "\n",
    "save_checkpoint({\n",
    "            'epoch': epoch + 1,\n",
    "            'arch': 'document-encoder',\n",
    "            'state_dict': model.state_dict(),\n",
    "            'best_prec1': best_prec1,\n",
    "            'optimizer' : optimizer.state_dict(),\n",
    "        }, is_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saved_model = 'model_best.pth.tar'\n",
    "#         if os.path.isfile(saved_model):\n",
    "#             print(\"=> loading checkpoint '{}'\".format(saved_model))\n",
    "#             checkpoint = torch.load(saved_model)\n",
    "#             args.start_epoch = checkpoint['epoch']\n",
    "#             best_prec1 = checkpoint['best_prec1']\n",
    "#             model.load_state_dict(checkpoint['state_dict'])\n",
    "#             optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "#             print(\"=> loaded checkpoint '{}' (epoch {})\"\n",
    "#                   .format(args.resume, checkpoint['epoch']))\n",
    "#         else:\n",
    "#             print(\"=> no checkpoint found at '{}'\".format(args.resume))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found pickled GloVe file. Loading...\n",
      "Done. 2195875 words loaded!\n",
      "Wall time: 44.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "data_q = SquadDataset(squad_file_path, glove_file_path, target='question')\n",
    "\n",
    "train_loader_q = DataLoader(data_q, batch_size=batch_size,\n",
    "                        shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestionEncoderLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers):\n",
    "        super(QuestionEncoderLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden1, num_layers)\n",
    "        \n",
    "        # Does the fact that they explicitly define a Q' (which is a whole matrix)\n",
    "        \n",
    "        self.lin = nn.Linear(hidden1, output_size)\n",
    "        self.tanh = nn.Tanh() \n",
    "        # sizes remain the same. Use input_size for both?\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.lstm(x)[0]\n",
    "        out = self.lin(out)\n",
    "        out = self.tanh(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_q = QuestionEncoderLSTM(input_size=input_size, hidden_size=hidden1, \n",
    "                              output_size=output_size, num_layers=num_layers)\n",
    "\n",
    "model_q.cuda()\n",
    "lossfun_q = nn.MSELoss()\n",
    "\n",
    "optimizer_q = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step[100/2737], Loss: 0.0035\n",
      "Epoch [1/10], Step[200/2737], Loss: 0.0033\n",
      "Epoch [1/10], Step[300/2737], Loss: 0.0033\n",
      "Epoch [1/10], Step[400/2737], Loss: 0.0034\n",
      "Epoch [1/10], Step[500/2737], Loss: 0.0032\n",
      "Epoch [1/10], Step[600/2737], Loss: 0.0032\n",
      "Epoch [1/10], Step[700/2737], Loss: 0.0034\n",
      "Epoch [1/10], Step[800/2737], Loss: 0.0034\n",
      "Epoch [1/10], Step[900/2737], Loss: 0.0033\n",
      "Epoch [1/10], Step[1000/2737], Loss: 0.0034\n",
      "Epoch [1/10], Step[1100/2737], Loss: 0.0033\n",
      "Epoch [1/10], Step[1200/2737], Loss: 0.0032\n",
      "Epoch [1/10], Step[1300/2737], Loss: 0.0033\n",
      "Epoch [1/10], Step[1400/2737], Loss: 0.0034\n",
      "Epoch [1/10], Step[1500/2737], Loss: 0.0031\n",
      "Epoch [1/10], Step[1600/2737], Loss: 0.0035\n",
      "Epoch [1/10], Step[1700/2737], Loss: 0.0036\n",
      "Epoch [1/10], Step[1800/2737], Loss: 0.0032\n",
      "Epoch [1/10], Step[1900/2737], Loss: 0.0031\n",
      "Epoch [1/10], Step[2000/2737], Loss: 0.0032\n",
      "Epoch [1/10], Step[2100/2737], Loss: 0.0032\n",
      "Epoch [1/10], Step[2200/2737], Loss: 0.0032\n",
      "Epoch [1/10], Step[2300/2737], Loss: 0.0032\n",
      "Epoch [1/10], Step[2400/2737], Loss: 0.0034\n",
      "Epoch [1/10], Step[2500/2737], Loss: 0.0033\n",
      "Epoch [1/10], Step[2600/2737], Loss: 0.0032\n",
      "Epoch [1/10], Step[2700/2737], Loss: 0.0032\n",
      "Epoch [2/10], Step[100/2737], Loss: 0.0030\n",
      "Epoch [2/10], Step[200/2737], Loss: 0.0033\n",
      "Epoch [2/10], Step[300/2737], Loss: 0.0033\n",
      "Epoch [2/10], Step[400/2737], Loss: 0.0032\n",
      "Epoch [2/10], Step[500/2737], Loss: 0.0033\n",
      "Epoch [2/10], Step[600/2737], Loss: 0.0035\n",
      "Epoch [2/10], Step[700/2737], Loss: 0.0034\n",
      "Epoch [2/10], Step[800/2737], Loss: 0.0033\n",
      "Epoch [2/10], Step[900/2737], Loss: 0.0033\n",
      "Epoch [2/10], Step[1000/2737], Loss: 0.0034\n",
      "Epoch [2/10], Step[1100/2737], Loss: 0.0033\n",
      "Epoch [2/10], Step[1200/2737], Loss: 0.0035\n",
      "Epoch [2/10], Step[1300/2737], Loss: 0.0035\n",
      "Epoch [2/10], Step[1400/2737], Loss: 0.0034\n",
      "Epoch [2/10], Step[1500/2737], Loss: 0.0032\n",
      "Epoch [2/10], Step[1600/2737], Loss: 0.0033\n",
      "Epoch [2/10], Step[1700/2737], Loss: 0.0036\n",
      "Epoch [2/10], Step[1800/2737], Loss: 0.0035\n",
      "Epoch [2/10], Step[1900/2737], Loss: 0.0034\n",
      "Epoch [2/10], Step[2000/2737], Loss: 0.0032\n",
      "Epoch [2/10], Step[2100/2737], Loss: 0.0035\n",
      "Epoch [2/10], Step[2200/2737], Loss: 0.0034\n",
      "Epoch [2/10], Step[2300/2737], Loss: 0.0032\n",
      "Epoch [2/10], Step[2400/2737], Loss: 0.0032\n",
      "Epoch [2/10], Step[2500/2737], Loss: 0.0034\n",
      "Epoch [2/10], Step[2600/2737], Loss: 0.0032\n",
      "Epoch [2/10], Step[2700/2737], Loss: 0.0035\n",
      "Epoch [3/10], Step[100/2737], Loss: 0.0031\n",
      "Epoch [3/10], Step[200/2737], Loss: 0.0035\n",
      "Epoch [3/10], Step[300/2737], Loss: 0.0034\n",
      "Epoch [3/10], Step[400/2737], Loss: 0.0033\n",
      "Epoch [3/10], Step[500/2737], Loss: 0.0031\n",
      "Epoch [3/10], Step[600/2737], Loss: 0.0034\n",
      "Epoch [3/10], Step[700/2737], Loss: 0.0034\n",
      "Epoch [3/10], Step[800/2737], Loss: 0.0033\n",
      "Epoch [3/10], Step[900/2737], Loss: 0.0033\n",
      "Epoch [3/10], Step[1000/2737], Loss: 0.0036\n",
      "Epoch [3/10], Step[1100/2737], Loss: 0.0033\n",
      "Epoch [3/10], Step[1200/2737], Loss: 0.0035\n",
      "Epoch [3/10], Step[1300/2737], Loss: 0.0034\n",
      "Epoch [3/10], Step[1400/2737], Loss: 0.0033\n",
      "Epoch [3/10], Step[1500/2737], Loss: 0.0037\n",
      "Epoch [3/10], Step[1600/2737], Loss: 0.0032\n",
      "Epoch [3/10], Step[1700/2737], Loss: 0.0034\n",
      "Epoch [3/10], Step[1800/2737], Loss: 0.0035\n",
      "Epoch [3/10], Step[1900/2737], Loss: 0.0034\n",
      "Epoch [3/10], Step[2000/2737], Loss: 0.0033\n",
      "Epoch [3/10], Step[2100/2737], Loss: 0.0033\n",
      "Epoch [3/10], Step[2200/2737], Loss: 0.0034\n",
      "Epoch [3/10], Step[2300/2737], Loss: 0.0033\n",
      "Epoch [3/10], Step[2400/2737], Loss: 0.0035\n",
      "Epoch [3/10], Step[2500/2737], Loss: 0.0034\n",
      "Epoch [3/10], Step[2600/2737], Loss: 0.0032\n",
      "Epoch [3/10], Step[2700/2737], Loss: 0.0035\n",
      "Epoch [4/10], Step[100/2737], Loss: 0.0033\n",
      "Epoch [4/10], Step[200/2737], Loss: 0.0032\n",
      "Epoch [4/10], Step[300/2737], Loss: 0.0034\n",
      "Epoch [4/10], Step[400/2737], Loss: 0.0036\n",
      "Epoch [4/10], Step[500/2737], Loss: 0.0031\n",
      "Epoch [4/10], Step[600/2737], Loss: 0.0033\n",
      "Epoch [4/10], Step[700/2737], Loss: 0.0034\n",
      "Epoch [4/10], Step[800/2737], Loss: 0.0033\n",
      "Epoch [4/10], Step[900/2737], Loss: 0.0035\n",
      "Epoch [4/10], Step[1000/2737], Loss: 0.0033\n",
      "Epoch [4/10], Step[1100/2737], Loss: 0.0033\n",
      "Epoch [4/10], Step[1200/2737], Loss: 0.0033\n",
      "Epoch [4/10], Step[1300/2737], Loss: 0.0033\n",
      "Epoch [4/10], Step[1400/2737], Loss: 0.0034\n",
      "Epoch [4/10], Step[1500/2737], Loss: 0.0033\n",
      "Epoch [4/10], Step[1600/2737], Loss: 0.0032\n",
      "Epoch [4/10], Step[1700/2737], Loss: 0.0033\n",
      "Epoch [4/10], Step[1800/2737], Loss: 0.0032\n",
      "Epoch [4/10], Step[1900/2737], Loss: 0.0032\n",
      "Epoch [4/10], Step[2000/2737], Loss: 0.0034\n",
      "Epoch [4/10], Step[2100/2737], Loss: 0.0032\n",
      "Epoch [4/10], Step[2200/2737], Loss: 0.0032\n",
      "Epoch [4/10], Step[2300/2737], Loss: 0.0033\n",
      "Epoch [4/10], Step[2400/2737], Loss: 0.0033\n",
      "Epoch [4/10], Step[2500/2737], Loss: 0.0033\n",
      "Epoch [4/10], Step[2600/2737], Loss: 0.0033\n",
      "Epoch [4/10], Step[2700/2737], Loss: 0.0035\n",
      "Epoch [5/10], Step[100/2737], Loss: 0.0034\n",
      "Epoch [5/10], Step[200/2737], Loss: 0.0032\n",
      "Epoch [5/10], Step[300/2737], Loss: 0.0033\n",
      "Epoch [5/10], Step[400/2737], Loss: 0.0033\n",
      "Epoch [5/10], Step[500/2737], Loss: 0.0033\n",
      "Epoch [5/10], Step[600/2737], Loss: 0.0034\n",
      "Epoch [5/10], Step[700/2737], Loss: 0.0034\n",
      "Epoch [5/10], Step[800/2737], Loss: 0.0035\n",
      "Epoch [5/10], Step[900/2737], Loss: 0.0031\n",
      "Epoch [5/10], Step[1000/2737], Loss: 0.0034\n",
      "Epoch [5/10], Step[1100/2737], Loss: 0.0034\n",
      "Epoch [5/10], Step[1200/2737], Loss: 0.0032\n",
      "Epoch [5/10], Step[1300/2737], Loss: 0.0034\n",
      "Epoch [5/10], Step[1400/2737], Loss: 0.0034\n",
      "Epoch [5/10], Step[1500/2737], Loss: 0.0032\n",
      "Epoch [5/10], Step[1600/2737], Loss: 0.0033\n",
      "Epoch [5/10], Step[1700/2737], Loss: 0.0032\n",
      "Epoch [5/10], Step[1800/2737], Loss: 0.0033\n",
      "Epoch [5/10], Step[1900/2737], Loss: 0.0032\n",
      "Epoch [5/10], Step[2000/2737], Loss: 0.0032\n",
      "Epoch [5/10], Step[2100/2737], Loss: 0.0034\n",
      "Epoch [5/10], Step[2200/2737], Loss: 0.0033\n",
      "Epoch [5/10], Step[2300/2737], Loss: 0.0033\n",
      "Epoch [5/10], Step[2400/2737], Loss: 0.0033\n",
      "Epoch [5/10], Step[2500/2737], Loss: 0.0033\n",
      "Epoch [5/10], Step[2600/2737], Loss: 0.0034\n",
      "Epoch [5/10], Step[2700/2737], Loss: 0.0035\n",
      "Epoch [6/10], Step[100/2737], Loss: 0.0034\n",
      "Epoch [6/10], Step[200/2737], Loss: 0.0034\n",
      "Epoch [6/10], Step[300/2737], Loss: 0.0032\n",
      "Epoch [6/10], Step[400/2737], Loss: 0.0034\n",
      "Epoch [6/10], Step[500/2737], Loss: 0.0033\n",
      "Epoch [6/10], Step[600/2737], Loss: 0.0033\n",
      "Epoch [6/10], Step[700/2737], Loss: 0.0033\n",
      "Epoch [6/10], Step[800/2737], Loss: 0.0031\n",
      "Epoch [6/10], Step[900/2737], Loss: 0.0033\n",
      "Epoch [6/10], Step[1000/2737], Loss: 0.0033\n",
      "Epoch [6/10], Step[1100/2737], Loss: 0.0034\n",
      "Epoch [6/10], Step[1200/2737], Loss: 0.0031\n",
      "Epoch [6/10], Step[1300/2737], Loss: 0.0032\n",
      "Epoch [6/10], Step[1400/2737], Loss: 0.0032\n",
      "Epoch [6/10], Step[1500/2737], Loss: 0.0032\n",
      "Epoch [6/10], Step[1600/2737], Loss: 0.0033\n",
      "Epoch [6/10], Step[1700/2737], Loss: 0.0036\n",
      "Epoch [6/10], Step[1800/2737], Loss: 0.0033\n",
      "Epoch [6/10], Step[1900/2737], Loss: 0.0033\n",
      "Epoch [6/10], Step[2000/2737], Loss: 0.0034\n",
      "Epoch [6/10], Step[2100/2737], Loss: 0.0032\n",
      "Epoch [6/10], Step[2200/2737], Loss: 0.0034\n",
      "Epoch [6/10], Step[2300/2737], Loss: 0.0033\n",
      "Epoch [6/10], Step[2400/2737], Loss: 0.0034\n",
      "Epoch [6/10], Step[2500/2737], Loss: 0.0033\n",
      "Epoch [6/10], Step[2600/2737], Loss: 0.0033\n",
      "Epoch [6/10], Step[2700/2737], Loss: 0.0034\n",
      "Epoch [7/10], Step[100/2737], Loss: 0.0033\n",
      "Epoch [7/10], Step[200/2737], Loss: 0.0034\n",
      "Epoch [7/10], Step[300/2737], Loss: 0.0034\n",
      "Epoch [7/10], Step[400/2737], Loss: 0.0031\n",
      "Epoch [7/10], Step[500/2737], Loss: 0.0033\n",
      "Epoch [7/10], Step[600/2737], Loss: 0.0034\n",
      "Epoch [7/10], Step[700/2737], Loss: 0.0033\n",
      "Epoch [7/10], Step[800/2737], Loss: 0.0034\n",
      "Epoch [7/10], Step[900/2737], Loss: 0.0034\n",
      "Epoch [7/10], Step[1000/2737], Loss: 0.0034\n",
      "Epoch [7/10], Step[1100/2737], Loss: 0.0033\n",
      "Epoch [7/10], Step[1200/2737], Loss: 0.0036\n",
      "Epoch [7/10], Step[1300/2737], Loss: 0.0033\n",
      "Epoch [7/10], Step[1400/2737], Loss: 0.0034\n",
      "Epoch [7/10], Step[1500/2737], Loss: 0.0034\n",
      "Epoch [7/10], Step[1600/2737], Loss: 0.0036\n",
      "Epoch [7/10], Step[1700/2737], Loss: 0.0033\n",
      "Epoch [7/10], Step[1800/2737], Loss: 0.0033\n",
      "Epoch [7/10], Step[1900/2737], Loss: 0.0032\n",
      "Epoch [7/10], Step[2000/2737], Loss: 0.0033\n",
      "Epoch [7/10], Step[2100/2737], Loss: 0.0035\n",
      "Epoch [7/10], Step[2200/2737], Loss: 0.0032\n",
      "Epoch [7/10], Step[2300/2737], Loss: 0.0033\n",
      "Epoch [7/10], Step[2400/2737], Loss: 0.0033\n",
      "Epoch [7/10], Step[2500/2737], Loss: 0.0032\n",
      "Epoch [7/10], Step[2600/2737], Loss: 0.0033\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10], Step[2700/2737], Loss: 0.0033\n",
      "Epoch [8/10], Step[100/2737], Loss: 0.0035\n",
      "Epoch [8/10], Step[200/2737], Loss: 0.0037\n",
      "Epoch [8/10], Step[300/2737], Loss: 0.0033\n",
      "Epoch [8/10], Step[400/2737], Loss: 0.0035\n",
      "Epoch [8/10], Step[500/2737], Loss: 0.0033\n",
      "Epoch [8/10], Step[600/2737], Loss: 0.0034\n",
      "Epoch [8/10], Step[700/2737], Loss: 0.0034\n",
      "Epoch [8/10], Step[800/2737], Loss: 0.0034\n",
      "Epoch [8/10], Step[900/2737], Loss: 0.0032\n",
      "Epoch [8/10], Step[1000/2737], Loss: 0.0032\n",
      "Epoch [8/10], Step[1100/2737], Loss: 0.0034\n",
      "Epoch [8/10], Step[1200/2737], Loss: 0.0032\n",
      "Epoch [8/10], Step[1300/2737], Loss: 0.0033\n",
      "Epoch [8/10], Step[1400/2737], Loss: 0.0032\n",
      "Epoch [8/10], Step[1500/2737], Loss: 0.0033\n",
      "Epoch [8/10], Step[1600/2737], Loss: 0.0035\n",
      "Epoch [8/10], Step[1700/2737], Loss: 0.0031\n",
      "Epoch [8/10], Step[1800/2737], Loss: 0.0035\n",
      "Epoch [8/10], Step[1900/2737], Loss: 0.0035\n",
      "Epoch [8/10], Step[2000/2737], Loss: 0.0033\n",
      "Epoch [8/10], Step[2100/2737], Loss: 0.0033\n",
      "Epoch [8/10], Step[2200/2737], Loss: 0.0033\n",
      "Epoch [8/10], Step[2300/2737], Loss: 0.0035\n",
      "Epoch [8/10], Step[2400/2737], Loss: 0.0035\n",
      "Epoch [8/10], Step[2500/2737], Loss: 0.0032\n",
      "Epoch [8/10], Step[2600/2737], Loss: 0.0031\n",
      "Epoch [8/10], Step[2700/2737], Loss: 0.0033\n",
      "Epoch [9/10], Step[100/2737], Loss: 0.0034\n",
      "Epoch [9/10], Step[200/2737], Loss: 0.0034\n",
      "Epoch [9/10], Step[300/2737], Loss: 0.0034\n",
      "Epoch [9/10], Step[400/2737], Loss: 0.0033\n",
      "Epoch [9/10], Step[500/2737], Loss: 0.0033\n",
      "Epoch [9/10], Step[600/2737], Loss: 0.0034\n",
      "Epoch [9/10], Step[700/2737], Loss: 0.0030\n",
      "Epoch [9/10], Step[800/2737], Loss: 0.0032\n",
      "Epoch [9/10], Step[900/2737], Loss: 0.0032\n",
      "Epoch [9/10], Step[1000/2737], Loss: 0.0032\n",
      "Epoch [9/10], Step[1100/2737], Loss: 0.0034\n",
      "Epoch [9/10], Step[1200/2737], Loss: 0.0035\n",
      "Epoch [9/10], Step[1300/2737], Loss: 0.0033\n",
      "Epoch [9/10], Step[1400/2737], Loss: 0.0035\n",
      "Epoch [9/10], Step[1500/2737], Loss: 0.0032\n",
      "Epoch [9/10], Step[1600/2737], Loss: 0.0035\n",
      "Epoch [9/10], Step[1700/2737], Loss: 0.0033\n",
      "Epoch [9/10], Step[1800/2737], Loss: 0.0032\n",
      "Epoch [9/10], Step[1900/2737], Loss: 0.0032\n",
      "Epoch [9/10], Step[2000/2737], Loss: 0.0034\n",
      "Epoch [9/10], Step[2100/2737], Loss: 0.0031\n",
      "Epoch [9/10], Step[2200/2737], Loss: 0.0032\n",
      "Epoch [9/10], Step[2300/2737], Loss: 0.0032\n",
      "Epoch [9/10], Step[2400/2737], Loss: 0.0033\n",
      "Epoch [9/10], Step[2500/2737], Loss: 0.0033\n",
      "Epoch [9/10], Step[2600/2737], Loss: 0.0033\n",
      "Epoch [9/10], Step[2700/2737], Loss: 0.0034\n",
      "Epoch [10/10], Step[100/2737], Loss: 0.0033\n",
      "Epoch [10/10], Step[200/2737], Loss: 0.0033\n",
      "Epoch [10/10], Step[300/2737], Loss: 0.0032\n",
      "Epoch [10/10], Step[400/2737], Loss: 0.0034\n",
      "Epoch [10/10], Step[500/2737], Loss: 0.0034\n",
      "Epoch [10/10], Step[600/2737], Loss: 0.0035\n",
      "Epoch [10/10], Step[700/2737], Loss: 0.0033\n",
      "Epoch [10/10], Step[800/2737], Loss: 0.0034\n",
      "Epoch [10/10], Step[900/2737], Loss: 0.0033\n",
      "Epoch [10/10], Step[1000/2737], Loss: 0.0034\n",
      "Epoch [10/10], Step[1100/2737], Loss: 0.0034\n",
      "Epoch [10/10], Step[1200/2737], Loss: 0.0035\n",
      "Epoch [10/10], Step[1300/2737], Loss: 0.0031\n",
      "Epoch [10/10], Step[1400/2737], Loss: 0.0035\n",
      "Epoch [10/10], Step[1500/2737], Loss: 0.0035\n",
      "Epoch [10/10], Step[1600/2737], Loss: 0.0032\n",
      "Epoch [10/10], Step[1700/2737], Loss: 0.0032\n",
      "Epoch [10/10], Step[1800/2737], Loss: 0.0035\n",
      "Epoch [10/10], Step[1900/2737], Loss: 0.0034\n",
      "Epoch [10/10], Step[2000/2737], Loss: 0.0035\n",
      "Epoch [10/10], Step[2100/2737], Loss: 0.0036\n",
      "Epoch [10/10], Step[2200/2737], Loss: 0.0032\n",
      "Epoch [10/10], Step[2300/2737], Loss: 0.0033\n",
      "Epoch [10/10], Step[2400/2737], Loss: 0.0034\n",
      "Epoch [10/10], Step[2500/2737], Loss: 0.0034\n",
      "Epoch [10/10], Step[2600/2737], Loss: 0.0033\n",
      "Epoch [10/10], Step[2700/2737], Loss: 0.0032\n",
      "Wall time: 3h 25min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, data_batch in enumerate(train_loader_q):\n",
    "        x = data_batch['question'].float()\n",
    "        x = Variable(data_batch['question'].float())\n",
    "        x = x.cuda()\n",
    "        y = x\n",
    "\n",
    "        output = model_q(x)\n",
    "    \n",
    "        optimizer_q.zero_grad()\n",
    "\n",
    "        loss_q = lossfun_q(output, y)\n",
    "        loss_q.backward()\n",
    "\n",
    "        optimizer_q.step()\n",
    "        if (i+1)%100 == 0:\n",
    "            print('Epoch [%d/%d], Step[%d/%d], Loss: %0.4f'\n",
    "            %(epoch+1, num_epochs, i+1, len(data)//batch_size, loss_q.data[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_prec1 = loss_q.data[0]\n",
    "is_best = True\n",
    "\n",
    "save_checkpoint({\n",
    "            'epoch': epoch + 1,\n",
    "            'arch': 'question-encoder',\n",
    "            'state_dict': model_q.state_dict(),\n",
    "            'best_prec1': best_prec1,\n",
    "            'optimizer' : optimizer_q.state_dict(),\n",
    "        }, is_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found pickled GloVe file. Loading...\n",
      "Done. 2195875 words loaded!\n",
      "Wall time: 1min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "dataloader_q = DataLoader(data_q, batch_size=batch_size,\n",
    "                        shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87599"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "cuda runtime error (2) : out of memory at d:\\pytorch\\pytorch\\torch\\lib\\thc\\generic/THCStorage.cu:58",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-122-b42511e4489e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'question'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_q\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0moutput_matrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mQ\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_matrix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\envs\\deep-learning-env\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    323\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 325\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    326\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-63-d28ec448717c>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\envs\\deep-learning-env\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    323\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 325\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    326\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\envs\\deep-learning-env\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    167\u001b[0m             \u001b[0mflat_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mflat_weight\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m         )\n\u001b[1;32m--> 169\u001b[1;33m         \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall_weights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    170\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_packed\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPackedSequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\envs\\deep-learning-env\\lib\\site-packages\\torch\\nn\\_functions\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(input, *fargs, **fkwargs)\u001b[0m\n\u001b[0;32m    383\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mhack_onnx_rnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mfargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 385\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mfargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    386\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    387\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\envs\\deep-learning-env\\lib\\site-packages\\torch\\autograd\\function.py\u001b[0m in \u001b[0;36m_do_forward\u001b[1;34m(self, *input)\u001b[0m\n\u001b[0;32m    326\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nested_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    327\u001b[0m         \u001b[0mflat_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_iter_variables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m         \u001b[0mflat_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNestedIOFunction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mflat_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    329\u001b[0m         \u001b[0mnested_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nested_output\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m         \u001b[0mnested_variables\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_unflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mflat_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nested_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\envs\\deep-learning-env\\lib\\site-packages\\torch\\autograd\\function.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    348\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    349\u001b[0m         \u001b[0mnested_tensors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_map_variable_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nested_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 350\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_extended\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnested_tensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    351\u001b[0m         \u001b[1;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nested_input\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    352\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nested_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\envs\\deep-learning-env\\lib\\site-packages\\torch\\nn\\_functions\\rnn.py\u001b[0m in \u001b[0;36mforward_extended\u001b[1;34m(self, input, weight, hx)\u001b[0m\n\u001b[0;32m    292\u001b[0m             \u001b[0mhy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mh\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mhx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 294\u001b[1;33m         \u001b[0mcudnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    295\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    296\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_for_backward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\envs\\deep-learning-env\\lib\\site-packages\\torch\\backends\\cudnn\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(fn, input, hx, weight, output, hy)\u001b[0m\n\u001b[0;32m    289\u001b[0m                 \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbyref\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreserve_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    290\u001b[0m             ))\n\u001b[1;32m--> 291\u001b[1;33m             \u001b[0mfn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreserve\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mByteTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreserve_size\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    292\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    293\u001b[0m             check_error(lib.cudnnRNNForwardTraining(\n",
      "\u001b[1;31mRuntimeError\u001b[0m: cuda runtime error (2) : out of memory at d:\\pytorch\\pytorch\\torch\\lib\\thc\\generic/THCStorage.cu:58"
     ]
    }
   ],
   "source": [
    "model_q.eval()\n",
    "\n",
    "Q = np.zeros([0, 600, 300])\n",
    "\n",
    "for i, data_batch in enumerate(dataloader_q):\n",
    "    print(i, i*batch_size)\n",
    "    \n",
    "    x = data_batch['question'].float()\n",
    "    x = Variable(data_batch['question'].float())\n",
    "    x = x.cuda()\n",
    "    output = model_q(x)\n",
    "    output_matrix = output.cpu().data.numpy()\n",
    "    np.concatenate((Q, output_matrix))\n",
    "    print(Q.shape)\n",
    "    if i==4:\n",
    "        \n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-80-65c21bdb7580>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mQ\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m87599\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m600\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m300\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "Q = np.zeros([87599, 600, 300])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.zeros([0,600,300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 18.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "a[0:32,:,:] = b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = np.concatenate((a,b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 600, 300)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.cuda.FloatTensor"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = x.cpu().data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 600, 300)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 600, 300])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.data.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
