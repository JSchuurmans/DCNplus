{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to run\n",
    "Reference to src folder for correct functions.\n",
    "\n",
    "## Notebook summary\n",
    "In this notebook, we perform the necessary data preprocessing. This consists of the following components:\n",
    "\n",
    "    - Loading the SQuAD data\n",
    "    - Tokenization with Stanford CoreNLP\n",
    "    - Embeddings with GloVe, pretrained on 840B Common Crawl, fixed\n",
    "\n",
    "In the end, we require a 2 x n array containing the input data and target for each question-answer pair. The target here is [UNRESOLVED, see questions]. The input data for each example consists of a paragraph and a question, and each are encoded in word vectors. This means both are matrices of p x l and q x l, where p is the number of words in the paragraph, l the length of the word embeddings, and q the number of words in the question. Whether these can be concatenated or should be separate items in an array is [UNRESOLVED, see questions]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions to resolve:\n",
    "\n",
    "- Should the target answer in training be text or two indices? Same goes for the model output. \n",
    "    - Assumption: should be text\n",
    "\n",
    "It seems that the model should output start and end indices for the span of the answer, but is evaluated on the words contained in the span. If this is the case, there has to be a step between model output and evaluation, where the two indices are converted to words in the span. \n",
    "\n",
    "- Where should this conversion from indices to words take place?\n",
    "\n",
    "- Can the document and question embedding matrices be concatenated or should they be passed as separate list items?\n",
    "- Why do we need the answer_start flag if we only match the output text with the target text?\n",
    "- Should the target answer texts be tokenized too?\n",
    "- Should we make full words out of tokenized contractions? (e.g. you're -> you, are | you're -> you, 're)\n",
    "    - Assumption: No, we stick with the original tokens and hope that they're in GloVe\n",
    "    - https://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html\n",
    "    - https://stanfordnlp.github.io/CoreNLP/tokenize.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_file_path = \"../data/glove.840B.300d.txt\"\n",
    "squad_file_path = '../data/train-v1.1.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.dataset import SquadDataset\n",
    "from src.preprocessing import Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'C:\\\\ProgramData\\\\Anaconda3\\\\python36.zip', 'C:\\\\ProgramData\\\\Anaconda3\\\\DLLs', 'C:\\\\ProgramData\\\\Anaconda3\\\\lib', 'C:\\\\ProgramData\\\\Anaconda3', 'C:\\\\ProgramData\\\\Anaconda3\\\\lib\\\\site-packages', 'C:\\\\ProgramData\\\\Anaconda3\\\\lib\\\\site-packages\\\\win32', 'C:\\\\ProgramData\\\\Anaconda3\\\\lib\\\\site-packages\\\\win32\\\\lib', 'C:\\\\ProgramData\\\\Anaconda3\\\\lib\\\\site-packages\\\\Pythonwin', 'C:\\\\ProgramData\\\\Anaconda3\\\\lib\\\\site-packages\\\\IPython\\\\extensions', 'C:\\\\Users\\\\jetze\\\\.ipython', '../']\n"
     ]
    }
   ],
   "source": [
    "#print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found pickled GloVe file. Loading...\n",
      "Done. 2195875 words loaded!\n"
     ]
    }
   ],
   "source": [
    "data = SquadDataset(squad_file_path, glove_file_path,'text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87599"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found pickled GloVe file. Loading...\n",
      "Done. 2195875 words loaded!\n"
     ]
    }
   ],
   "source": [
    "q = SquadDataset(squad_file_path, glove_file_path,'question')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87599"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found pickled GloVe file. Loading...\n",
      "Done. 2195875 words loaded!\n"
     ]
    }
   ],
   "source": [
    "a = SquadDataset(squad_file_path, glove_file_path,'answer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87599"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sandbox for preprocessing\n",
    "This is outdated -> check src folder for up to date function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from pycorenlp import StanfordCoreNLP\n",
    "\n",
    "nlp = StanfordCoreNLP('http://localhost:9001')\n",
    "# If server is offline, run the command below in Terminal from the stanford CoreNLP folder\n",
    "# java -mx4g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9001 -timeout 15000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "squad = pd.read_json('../data/train-v1.1.json', orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(442, 2)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so we need to extract a couple of things:\n",
    "\n",
    "- answers, which are just texts (that should be tokenized? [UNRESOLVED, see questions])\n",
    "- questions, which should be tokenized and embedded\n",
    "- paragraphs, which should be tokenized and embedded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stanford CoreNLP Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text, annotator=nlp):\n",
    "    \"\"\"\"\n",
    "    Calls the Stanford CoreNLP Tokenizer running on a local server, which tokenizes the input text.\n",
    "    \n",
    "    Returns:\n",
    "    Tokenized text\n",
    "    \"\"\"\n",
    "    annotated_text = annotator.annotate(text, properties={'annotators': 'tokenize','outputFormat': 'json'})\n",
    "    tokenized_text = []\n",
    "    for token in annotated_text['tokens']:\n",
    "        word = token['word']\n",
    "        tokenized_text.append(word)\n",
    "        \n",
    "    return tokenized_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe word embeddings\n",
    "From the DCN paper:\n",
    "\n",
    "\"We use as GloVe word vectors pretrained on the 840B Common Crawl corpus (Pennington et al., 2014). We limit the vocabulary to words that are present in the Common Crawl corpus and set embeddings for out-of-vocabulary words to zero. Empirically, we found that training the embeddings consistently led to overfitting and subpar performance, and hence only report results with fixed word embeddings.\"\n",
    "\n",
    "When reading in the GloVe vectors, we found that some vectors were the wrong length and contained odd words (such as name@example.com) and values (such as '.'). We don't know whether this is intrinsic to the data or whether we import it wrong. Either way, out of the 2196016 total lines, 29 were of the wrong length. We therefore decided to drop those 29 vectors and set the embeddings for the corresponding words to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from preprocessing\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from pycorenlp import StanfordCoreNLP\n",
    "\n",
    "class Preprocessing():\n",
    "    \"\"\"\n",
    "    Class containing tokenization and embeddings functions, borrwoing from the Stanford\n",
    "    CoreNLP tokenizer and the pretrained GloVe word embeddings\n",
    "    \n",
    "    About the CoreNLP server: if server is offline, run the command below in\n",
    "    Terminal from the stanford CoreNLP folder:\n",
    "    \n",
    "    java -mx4g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9001 -timeout 15000\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def __init__(self, glove_file_path):\n",
    "        self.annotator = StanfordCoreNLP('http://localhost:9001')\n",
    "        self.embeddings = self.load_glove_embeddings(glove_file_path)\n",
    "        \n",
    "        \n",
    "    def load_glove_embeddings(self, file_path):\n",
    "        \"\"\"\n",
    "        Loads the glove word vectors from a textfile and parses it into a directory \n",
    "        with words and vectors.\n",
    "        \n",
    "        Returns:\n",
    "        A dictionary of words and corresponding vectors\n",
    "        \"\"\"\n",
    "        \n",
    "        cached_file = '../data/glove.pickle'\n",
    "        if os.path.isfile(cached_file):\n",
    "            print(\"Found pickled GloVe file. Loading...\")\n",
    "            with open(cached_file, 'rb') as handle:\n",
    "                embeddings_dict = pickle.load(handle)\n",
    "        else:\n",
    "            print(\"Loading GloVe model from .txt...\") #changed Glove to GloVe\n",
    "            with open(file_path, 'r', encoding='utf8') as f:\n",
    "                embeddings_dict = {}\n",
    "                cnt = 0\n",
    "                for i, line in enumerate(f):\n",
    "                    split_line = line.split()\n",
    "                    \n",
    "                    #skip aberrant lines\n",
    "                    if not len(split_line) == 301:\n",
    "                        continue\n",
    "                    \n",
    "                    word = split_line[0]\n",
    "                    embedding = no.array([float(val) for val in split_line[1:]]) # why from 1?\n",
    "                    embeddings_dict[word] = embedding\n",
    "                \n",
    "                with open('../data/glove.pickle', 'wb') as handle:\n",
    "                    print(\"Saving GloVes as pickle...\")\n",
    "                    pickle.dump(embeddings_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                \n",
    "            print(\"Done. {} words loaded!\".format(len(embeddings_dict)))\n",
    "            \n",
    "        return embeddings_dict\n",
    "    \n",
    "    \n",
    "    def preprocess(self, text):\n",
    "        \"\"\"\n",
    "        Tokenizes and applies word embeddings to a text. Also pads or cuts the whole\n",
    "        sequence to length 600.\n",
    "        \"\"\"\n",
    "        tokenized_text = self.tokenize(text)\n",
    "        embedded_text = self.embed(tokenized_text)\n",
    "        embedded_text = embedded_text[:600,:]\n",
    "        padded_embeddings = np.zeros([600, 300])\n",
    "        padded_embeddings[:embedded_text.shape[0], :embedded_text.shape[1]] = embedded_text\n",
    "        \n",
    "        return padded_embeddings\n",
    "    \n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        \"\"\"\n",
    "        Calls the Stanford CoreNLP Tokenizer running on a local server, which tokenizes \n",
    "        the input text.\n",
    "        \"\"\"\n",
    "        annotated_text = self.annotator.annotate(text,\n",
    "                                                properties = {'annotators': 'tokenize',\n",
    "                                                             'outputFormat': 'json'})\n",
    "        tokenized_text = []\n",
    "        for token in annotated_text['tokens']:\n",
    "            word = token['word']\n",
    "            tokenized_text.append(word)\n",
    "            \n",
    "        return tokenized_text\n",
    "    \n",
    "    \n",
    "    def embed(self, words):\n",
    "        \"\"\"\n",
    "        Takes words and returns corresponding GloVe word embeddings.\n",
    "        Returns a zero vector if no embedding is found.\n",
    "        \n",
    "        Returns:\n",
    "        List of word vectors\n",
    "        \"\"\"\n",
    "        word_vectors = np.zeros((len(words),300))\n",
    "        \n",
    "        for i, word in enumerate(words):\n",
    "            # Match word with vector\n",
    "            try:\n",
    "                vector = self.embeddings[word]\n",
    "            except KeyError:\n",
    "                # Set to zero vector if no match\n",
    "                vector = np.zeros(300)\n",
    "            \n",
    "            word_vectors[i] = vector\n",
    "            \n",
    "        return word_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "imports in a class:\n",
    "\n",
    "https://stackoverflow.com/questions/6861487/importing-modules-inside-python-class\n",
    "\n",
    "https://www.python.org/dev/peps/pep-0008/\n",
    "PEP-08:\n",
    "'Imports are always put at the top of the file, just after any module comments and docstrings, and before module globals and constants.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code above, what happens at the skip aberrant lines?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Add a check to verify that the amount of null vectors is relatively low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SquadDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset object for the Stanford Question Answering Dataset.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __inti__(self, data_file_path, glove_file_path, target):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            json_file (string): Path to the csv file with annotations.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.dataset = pd.read_json(data_file_path, orient='records')['data']\n",
    "        self.dataset = self.flatten_data(self.dataset)\n",
    "        self.preprocess = Preprocessing(glove_file_path)\n",
    "        self.target = target\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        item = self.dataset[idx]\n",
    "        \n",
    "        if self.target in ['text','question']:\n",
    "            data_point = item[self.target]\n",
    "            data_point = self.preprocess.preprocess(data_point)\n",
    "            sample = {self.target: torch.from_numpy(data_point)}\n",
    "        else:\n",
    "            data_point = item['answer']\n",
    "            sample = {self.target: data_point}\n",
    "            \n",
    "        return sample\n",
    "    \n",
    "    def flatten_data(self, data):\n",
    "        flat_data = []\n",
    "        for article in data:\n",
    "            for paragraph in article['paragraphs']:\n",
    "                for qa in paragraph['qas']:\n",
    "                    flat_data.append({'text': paragraph['context'],\n",
    "                                     'question': qa['question'],\n",
    "                                     'answer': qa['answers'][0]['text']})\n",
    "        return flat_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: update args in SquadDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: what happens in flatten?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
