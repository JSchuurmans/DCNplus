{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coattention encoder\n",
    "\n",
    "We've constructed encoding matrices for the documents and questions, D and Q. Each document (e.g. paragraph in SQUAD) has its own D and each question has its own Q. Matrix D is of shape 300 x m+1 where m is the number of words in the document. Matrix Q is of shape 300 x n+1 where n is the number of wordsd in the question. 300 is the length of the word vectors, and the +1 comes from the sentinel vector. Let's assume we have these matrices. For now we'll use dummies of the right shape to get the process right. Also, we'll start by doing this for one document and one question. Later, we'll extend this to the whole dataset."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 2,
=======
   "execution_count": 78,
>>>>>>> c97ad260fa3a3fbdbb19fc3542599c844b995584
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 20\n",
    "m = 40\n",
    "l = 200\n",
    "\n",
    "Q = Variable(torch.Tensor(l, n+1).zero_(), requires_grad=False)\n",
    "D = Variable(torch.Tensor(l, m+1).zero_(), requires_grad=False)\n",
    "# We don't need to calculate gradients for this part of the process, right?"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 3,
=======
   "execution_count": 79,
>>>>>>> c97ad260fa3a3fbdbb19fc3542599c844b995584
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([41, 21])"
      ]
     },
<<<<<<< HEAD
     "execution_count": 3,
=======
     "execution_count": 79,
>>>>>>> c97ad260fa3a3fbdbb19fc3542599c844b995584
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate affinity matrix\n",
    "L = D.transpose(0, 1).matmul(Q)\n",
    "L.size()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 4,
=======
   "execution_count": 80,
>>>>>>> c97ad260fa3a3fbdbb19fc3542599c844b995584
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate attention weights A for Q and D\n",
    "softmax = nn.Softmax(dim=1)\n",
    "\n",
    "A_q = softmax(L)\n",
    "A_d = softmax(L.transpose(0,1))"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 5,
=======
   "execution_count": 81,
>>>>>>> c97ad260fa3a3fbdbb19fc3542599c844b995584
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values per row in A_d sum to 0.9999999403953552 and A_d has shape [21, 41]. \n",
      "Values per row in       A_q sum to 1.0 and A_q has shape [41, 21]\n"
     ]
    }
   ],
   "source": [
    "# check that each row sums to 1\n",
    "d_sum = A_d[0].sum().data[0]\n",
    "q_sum = A_q[0].sum().data[0]\n",
    "d_size = list(A_d.size())\n",
    "q_size = list(A_q.size())\n",
    "\n",
    "print(\"Values per row in A_d sum to {} and A_d has shape {}. \\nValues per row in \\\n",
    "      A_q sum to {} and A_q has shape {}\".format(d_sum, d_size, q_sum, q_size))\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 6,
=======
   "execution_count": 82,
>>>>>>> c97ad260fa3a3fbdbb19fc3542599c844b995584
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([200, 21])"
      ]
     },
<<<<<<< HEAD
     "execution_count": 6,
=======
     "execution_count": 82,
>>>>>>> c97ad260fa3a3fbdbb19fc3542599c844b995584
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute attention context of the document wrt each word in the question \n",
    "# (notation C_q is confusing)\n",
    "\n",
    "C_q = D.matmul(A_q)\n",
    "C_q.size()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 7,
=======
   "execution_count": 83,
>>>>>>> c97ad260fa3a3fbdbb19fc3542599c844b995584
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([400, 41])"
      ]
     },
<<<<<<< HEAD
     "execution_count": 7,
=======
     "execution_count": 83,
>>>>>>> c97ad260fa3a3fbdbb19fc3542599c844b995584
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute attention context of question wrt each word in the document and\n",
    "# compute the attention context of the previous attention context wrt each\n",
    "# word in the document. This is the coattention context C_d.\n",
    "C_d = torch.cat((Q, C_q), dim=0).matmul(A_d)\n",
    "C_d.size()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 8,
=======
   "execution_count": 84,
>>>>>>> c97ad260fa3a3fbdbb19fc3542599c844b995584
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoattentionEncoderBiLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(CoattentionEncoderBiLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, \n",
    "                            bidirectional=True)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out = self.lstm(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A single input to the Bi-LSTM consists of an encoded word in the document (e.g. d of length 300 in D) concatenated with coattention context vector c of length 600 in C_d. So the input is of length 900?\n",
    "\n",
    "The length of the output vector is 600. \n",
    "\n",
    "the input consists of (seq_len, batch_size, input_size), of which input_size has to be predefined. \n",
    "the seq_len is the length of a document (set to 600), batch size is obvious, input_size is 900 (originally 300 for the length of a word vector, but we concatenate the document vector with a coattention vector of length 2x300)\n",
    "\n",
    "output consists of (seq_len, batch_size, hidden_size x num_directions). The hidden units are set to 200, and we have 2 directions. The output_size should then be 400. In the paper this is 2l, which should be 600. WTF???\n",
    "\n",
    "Should l have been 200 all along??? \n",
    "\n",
    "Let's assume for now that it is 200."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 9,
=======
   "execution_count": 110,
>>>>>>> c97ad260fa3a3fbdbb19fc3542599c844b995584
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 600 # Length of d (200) and c_d (400) concatenated\n",
    "hidden_size = 200\n",
    "\n",
    "num_layers = 2\n",
    "\n",
    "batch_size = 32\n",
    "learning_rate = 0.0007\n",
    "num_epochs = 2"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 10,
=======
   "execution_count": 111,
>>>>>>> c97ad260fa3a3fbdbb19fc3542599c844b995584
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CoattentionEncoderBiLSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers)\n",
    "\n",
    "model.cuda()\n",
    "lossfun = nn.MSELoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 13,
=======
   "execution_count": 112,
>>>>>>> c97ad260fa3a3fbdbb19fc3542599c844b995584
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacky way of adding another dimension and putting them in the right order\n",
    "# Can't find torch.reshape\n",
    "\n",
    "x = Variable(torch.Tensor(1, 3*l, m+1), requires_grad=False)\n",
    "x = x.add(a)\n",
    "x = x.transpose(0,2)\n",
    "x = x.transpose(1,2)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 14,
=======
   "execution_count": 113,
>>>>>>> c97ad260fa3a3fbdbb19fc3542599c844b995584
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([41, 1, 600])"
      ]
     },
<<<<<<< HEAD
     "execution_count": 14,
=======
     "execution_count": 113,
>>>>>>> c97ad260fa3a3fbdbb19fc3542599c844b995584
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 15,
=======
   "execution_count": 114,
>>>>>>> c97ad260fa3a3fbdbb19fc3542599c844b995584
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "input and target have different number of elements: input[41 x 1 x 400] has 16400 elements, while target[41 x 1 x 600] has 24600 elements at d:\\pytorch\\pytorch\\torch\\lib\\thcunn\\generic/MSECriterion.cu:15",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
<<<<<<< HEAD
      "\u001b[1;32m<ipython-input-15-269318a794f2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlossfun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
=======
      "\u001b[1;32m<ipython-input-114-269318a794f2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlossfun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
>>>>>>> c97ad260fa3a3fbdbb19fc3542599c844b995584
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\envs\\deep-learning-env\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    323\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 325\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    326\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\envs\\deep-learning-env\\lib\\site-packages\\torch\\nn\\modules\\loss.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    327\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m         \u001b[0m_assert_no_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 329\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize_average\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    330\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    331\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: input and target have different number of elements: input[41 x 1 x 400] has 16400 elements, while target[41 x 1 x 600] has 24600 elements at d:\\pytorch\\pytorch\\torch\\lib\\thcunn\\generic/MSECriterion.cu:15"
     ]
    }
   ],
   "source": [
    "\n",
    "x = x.cuda()\n",
    "y = x\n",
    "\n",
    "output = model(x)\n",
    "\n",
    "optimizer.zero_grad()\n",
    "\n",
    "loss = lossfun(output[0], y)\n",
    "loss.backward()\n",
    "\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
=======
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([200, 41])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
>>>>>>> c97ad260fa3a3fbdbb19fc3542599c844b995584
   "source": [
    "D.size()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
=======
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([400, 41])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
>>>>>>> c97ad260fa3a3fbdbb19fc3542599c844b995584
   "source": [
    "C_d.size()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 12,
=======
   "execution_count": 93,
>>>>>>> c97ad260fa3a3fbdbb19fc3542599c844b995584
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.cat((D, C_d)) "
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
=======
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 600, 41])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
>>>>>>> c97ad260fa3a3fbdbb19fc3542599c844b995584
   "source": [
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
