{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "\n",
    "from src.dataset import SquadDataset\n",
    "from src.preprocessing import Preprocessing\n",
    "\n",
    "# Clear memory\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook summary\n",
    "\n",
    "In this notebook we'll set up the model architectures required for the first encoders. These encode the words in the documents, and the words in the questions. Both questions and documents are initially encoded by an LSTM:\n",
    "    \n",
    "    d_t = LSTM_enc(d_t−1, x_t^D)\n",
    "    \n",
    "resulting in document encoding matrix\n",
    "\n",
    "$$ D = [d1, . . ., d_m, d∅] of L x (m+1)$$ dimensions\n",
    "\n",
    "and\n",
    "\n",
    "$$ q_t = LSTMenc(q_t−1, x_t^D) $$\n",
    "    \n",
    "resulting in intermediate question encoding matrix\n",
    "\n",
    "    Q' = [q_1, . . ., q_n, q∅] of L x (n+1) dimensions\n",
    "\n",
    "to which we then apply a nonlinearity\n",
    "\n",
    "    Q = tanh(W^(Q)Q_0 + b(Q)) of L x (n+1) dimensions\n",
    "\n",
    "Let's start!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "glove_file_path = \"../data/glove.840B.300d.txt\"\n",
    "squad_file_path = \"../data/train-v1.1.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentEncoderLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(DocumentEncoderLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.lstm(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do we have a single document encoding matrix for all documents, or an encoding matrix for each document? It seems we have one for each document, where L is the length of the transformed word vectors and m+1 is the number of words in the document plus a sentinel vector. \n",
    "\n",
    "The shape of the input of a neural net is always defined on the level of a single example, as the batch size may vary. The above would suggest that we feed the network word vectors for a whole document. We pass each word vector through the same LSTM and we obtain new, encoded vectors (which incorporate some of their surrounding context).\n",
    "\n",
    "This raises another question: how are we training this encoding? It seems we do not have a target to train on and therefore no error signal, at least in this section on its own. Just feeding the vectors through an LSTM with random weights seems a little pointless. It seems more likely that this is learned by going through the whole architecture. Does this mean that in order to test this we need to have the whole thing set up?\n",
    "\n",
    "After we have both encodings D and Q, we calculate affinity matrix L = (D.transpose Q). This makes it unlikely that the encoders are coupled to the whole network, since it is difficult (impossible?) to disentangle the error signal you backpropagate.\n",
    "\n",
    "SOLUTION: encoders are unsupervised, and they try to learn a mapping from x to x, e.g. they approximate the identity function. So we train the LSTM with backprop and pass our input along as targets. Conceptually, we have the word vectors, which encode meaning of single words. We pass these through an LSTM, which learns word context. So as output we get the same word meanings, which somehow also encapsulate word interactions because they have been through the LSTM. Is this correct??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "# Assuming that the LSTM takes one word at a time and the sizes stay the same through the encoder \n",
    "input_size = 300\n",
    "hidden_size = 300\n",
    "output_size = 300\n",
    "num_layers = 2\n",
    "batch_size = 8\n",
    "learning_rate = 0.0007\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup model\n",
    "model = DocumentEncoderLSTM(input_size, hidden_size, num_layers)\n",
    "model.cuda()\n",
    "lossfun = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we're encoding the data we are learning the identity function. This means we use input data x as our target. This is a 3D Tensor, and the go-to loss function CrossEntropyLoss expects a 2D Tensor (usually labels are 1D, for every example, so 2D). Should we flatten our x? On the other hand, as it's not really classes we're predicting, it might be more intuitive to use the MSE or something similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found pickled GloVe file. Loading...\n",
      "Done. 2195875 words loaded!\n"
     ]
    }
   ],
   "source": [
    "# Get data\n",
    "data = SquadDataset(squad_file_path, glove_file_path, target='text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step[100/10949], Loss: 0.0104\n",
      "Epoch [1/10], Step[200/10949], Loss: 0.0097\n",
      "Epoch [1/10], Step[300/10949], Loss: 0.0081\n",
      "Epoch [1/10], Step[400/10949], Loss: 0.0050\n",
      "Epoch [1/10], Step[500/10949], Loss: 0.0041\n",
      "Epoch [1/10], Step[600/10949], Loss: 0.0044\n",
      "Epoch [1/10], Step[700/10949], Loss: 0.0045\n",
      "Epoch [1/10], Step[800/10949], Loss: 0.0038\n",
      "Epoch [1/10], Step[900/10949], Loss: 0.0043\n",
      "Epoch [1/10], Step[1000/10949], Loss: 0.0039\n",
      "Epoch [1/10], Step[1100/10949], Loss: 0.0029\n",
      "Epoch [1/10], Step[1200/10949], Loss: 0.0032\n",
      "Epoch [1/10], Step[1300/10949], Loss: 0.0039\n",
      "Epoch [1/10], Step[1400/10949], Loss: 0.0031\n",
      "Epoch [1/10], Step[1500/10949], Loss: 0.0026\n",
      "Epoch [1/10], Step[1600/10949], Loss: 0.0025\n",
      "Epoch [1/10], Step[1700/10949], Loss: 0.0030\n",
      "Epoch [1/10], Step[1800/10949], Loss: 0.0033\n",
      "Epoch [1/10], Step[1900/10949], Loss: 0.0037\n",
      "Epoch [1/10], Step[2000/10949], Loss: 0.0024\n",
      "Epoch [1/10], Step[2100/10949], Loss: 0.0029\n",
      "Epoch [1/10], Step[2200/10949], Loss: 0.0033\n",
      "Epoch [1/10], Step[2300/10949], Loss: 0.0022\n",
      "Epoch [1/10], Step[2400/10949], Loss: 0.0029\n",
      "Epoch [1/10], Step[2500/10949], Loss: 0.0029\n",
      "Epoch [1/10], Step[2600/10949], Loss: 0.0025\n",
      "Epoch [1/10], Step[2700/10949], Loss: 0.0034\n",
      "Epoch [1/10], Step[2800/10949], Loss: 0.0023\n",
      "Epoch [1/10], Step[2900/10949], Loss: 0.0026\n",
      "Epoch [1/10], Step[3000/10949], Loss: 0.0026\n",
      "Epoch [1/10], Step[3100/10949], Loss: 0.0019\n",
      "Epoch [1/10], Step[3200/10949], Loss: 0.0032\n",
      "Epoch [1/10], Step[3300/10949], Loss: 0.0030\n",
      "Epoch [1/10], Step[3400/10949], Loss: 0.0027\n",
      "Epoch [1/10], Step[3500/10949], Loss: 0.0037\n",
      "Epoch [1/10], Step[3600/10949], Loss: 0.0031\n",
      "Epoch [1/10], Step[3700/10949], Loss: 0.0031\n",
      "Epoch [1/10], Step[3800/10949], Loss: 0.0030\n",
      "Epoch [1/10], Step[3900/10949], Loss: 0.0030\n",
      "Epoch [1/10], Step[4000/10949], Loss: 0.0030\n",
      "Epoch [1/10], Step[4100/10949], Loss: 0.0030\n",
      "Epoch [1/10], Step[4200/10949], Loss: 0.0032\n",
      "Epoch [1/10], Step[4300/10949], Loss: 0.0024\n",
      "Epoch [1/10], Step[4400/10949], Loss: 0.0025\n",
      "Epoch [1/10], Step[4500/10949], Loss: 0.0025\n",
      "Epoch [1/10], Step[4600/10949], Loss: 0.0025\n",
      "Epoch [1/10], Step[4700/10949], Loss: 0.0027\n",
      "Epoch [1/10], Step[4800/10949], Loss: 0.0031\n",
      "Epoch [1/10], Step[4900/10949], Loss: 0.0031\n",
      "Epoch [1/10], Step[5000/10949], Loss: 0.0026\n",
      "Epoch [1/10], Step[5100/10949], Loss: 0.0024\n",
      "Epoch [1/10], Step[5200/10949], Loss: 0.0024\n",
      "Epoch [1/10], Step[5300/10949], Loss: 0.0027\n",
      "Epoch [1/10], Step[5400/10949], Loss: 0.0032\n",
      "Epoch [1/10], Step[5500/10949], Loss: 0.0024\n",
      "Epoch [1/10], Step[5600/10949], Loss: 0.0028\n",
      "Epoch [1/10], Step[5700/10949], Loss: 0.0033\n",
      "Epoch [1/10], Step[5800/10949], Loss: 0.0031\n",
      "Epoch [1/10], Step[5900/10949], Loss: 0.0027\n",
      "Epoch [1/10], Step[6000/10949], Loss: 0.0028\n",
      "Epoch [1/10], Step[6100/10949], Loss: 0.0025\n",
      "Epoch [1/10], Step[6200/10949], Loss: 0.0022\n",
      "Epoch [1/10], Step[6300/10949], Loss: 0.0027\n",
      "Epoch [1/10], Step[6400/10949], Loss: 0.0033\n",
      "Epoch [1/10], Step[6500/10949], Loss: 0.0025\n",
      "Epoch [1/10], Step[6600/10949], Loss: 0.0023\n",
      "Epoch [1/10], Step[6700/10949], Loss: 0.0021\n",
      "Epoch [1/10], Step[6800/10949], Loss: 0.0026\n",
      "Epoch [1/10], Step[6900/10949], Loss: 0.0024\n",
      "Epoch [1/10], Step[7000/10949], Loss: 0.0024\n",
      "Epoch [1/10], Step[7100/10949], Loss: 0.0026\n",
      "Epoch [1/10], Step[7200/10949], Loss: 0.0019\n",
      "Epoch [1/10], Step[7300/10949], Loss: 0.0024\n",
      "Epoch [1/10], Step[7400/10949], Loss: 0.0022\n",
      "Epoch [1/10], Step[7500/10949], Loss: 0.0032\n",
      "Epoch [1/10], Step[7600/10949], Loss: 0.0021\n",
      "Epoch [1/10], Step[7700/10949], Loss: 0.0026\n",
      "Epoch [1/10], Step[7800/10949], Loss: 0.0027\n",
      "Epoch [1/10], Step[7900/10949], Loss: 0.0019\n",
      "Epoch [1/10], Step[8000/10949], Loss: 0.0023\n",
      "Epoch [1/10], Step[8100/10949], Loss: 0.0020\n",
      "Epoch [1/10], Step[8200/10949], Loss: 0.0021\n",
      "Epoch [1/10], Step[8300/10949], Loss: 0.0029\n",
      "Epoch [1/10], Step[8400/10949], Loss: 0.0023\n",
      "Epoch [1/10], Step[8500/10949], Loss: 0.0025\n",
      "Epoch [1/10], Step[8600/10949], Loss: 0.0023\n",
      "Epoch [1/10], Step[8700/10949], Loss: 0.0031\n",
      "Epoch [1/10], Step[8800/10949], Loss: 0.0019\n",
      "Epoch [1/10], Step[8900/10949], Loss: 0.0026\n",
      "Epoch [1/10], Step[9000/10949], Loss: 0.0031\n",
      "Epoch [1/10], Step[9100/10949], Loss: 0.0023\n",
      "Epoch [1/10], Step[9200/10949], Loss: 0.0021\n",
      "Epoch [1/10], Step[9300/10949], Loss: 0.0026\n",
      "Epoch [1/10], Step[9400/10949], Loss: 0.0022\n",
      "Epoch [1/10], Step[9500/10949], Loss: 0.0019\n",
      "Epoch [1/10], Step[9600/10949], Loss: 0.0024\n",
      "Epoch [1/10], Step[9700/10949], Loss: 0.0026\n",
      "Epoch [1/10], Step[9800/10949], Loss: 0.0024\n",
      "Epoch [1/10], Step[9900/10949], Loss: 0.0034\n",
      "Epoch [1/10], Step[10000/10949], Loss: 0.0028\n",
      "Epoch [1/10], Step[10100/10949], Loss: 0.0029\n",
      "Epoch [1/10], Step[10200/10949], Loss: 0.0025\n",
      "Epoch [1/10], Step[10300/10949], Loss: 0.0030\n",
      "Epoch [1/10], Step[10400/10949], Loss: 0.0026\n",
      "Epoch [1/10], Step[10500/10949], Loss: 0.0021\n",
      "Epoch [1/10], Step[10600/10949], Loss: 0.0021\n",
      "Epoch [1/10], Step[10700/10949], Loss: 0.0027\n",
      "Epoch [1/10], Step[10800/10949], Loss: 0.0026\n",
      "Epoch [1/10], Step[10900/10949], Loss: 0.0026\n",
      "Epoch [2/10], Step[100/10949], Loss: 0.0022\n",
      "Epoch [2/10], Step[200/10949], Loss: 0.0024\n",
      "Epoch [2/10], Step[300/10949], Loss: 0.0022\n",
      "Epoch [2/10], Step[400/10949], Loss: 0.0027\n",
      "Epoch [2/10], Step[500/10949], Loss: 0.0024\n",
      "Epoch [2/10], Step[600/10949], Loss: 0.0028\n",
      "Epoch [2/10], Step[700/10949], Loss: 0.0020\n",
      "Epoch [2/10], Step[800/10949], Loss: 0.0026\n",
      "Epoch [2/10], Step[900/10949], Loss: 0.0024\n",
      "Epoch [2/10], Step[1000/10949], Loss: 0.0028\n",
      "Epoch [2/10], Step[1100/10949], Loss: 0.0026\n",
      "Epoch [2/10], Step[1200/10949], Loss: 0.0022\n",
      "Epoch [2/10], Step[1300/10949], Loss: 0.0027\n",
      "Epoch [2/10], Step[1400/10949], Loss: 0.0023\n",
      "Epoch [2/10], Step[1500/10949], Loss: 0.0024\n",
      "Epoch [2/10], Step[1600/10949], Loss: 0.0027\n",
      "Epoch [2/10], Step[1700/10949], Loss: 0.0022\n",
      "Epoch [2/10], Step[1800/10949], Loss: 0.0026\n",
      "Epoch [2/10], Step[1900/10949], Loss: 0.0020\n",
      "Epoch [2/10], Step[2000/10949], Loss: 0.0025\n",
      "Epoch [2/10], Step[2100/10949], Loss: 0.0031\n",
      "Epoch [2/10], Step[2200/10949], Loss: 0.0029\n",
      "Epoch [2/10], Step[2300/10949], Loss: 0.0025\n",
      "Epoch [2/10], Step[2400/10949], Loss: 0.0021\n",
      "Epoch [2/10], Step[2500/10949], Loss: 0.0021\n",
      "Epoch [2/10], Step[2600/10949], Loss: 0.0028\n",
      "Epoch [2/10], Step[2700/10949], Loss: 0.0022\n",
      "Epoch [2/10], Step[2800/10949], Loss: 0.0022\n",
      "Epoch [2/10], Step[2900/10949], Loss: 0.0023\n",
      "Epoch [2/10], Step[3000/10949], Loss: 0.0019\n",
      "Epoch [2/10], Step[3100/10949], Loss: 0.0027\n",
      "Epoch [2/10], Step[3200/10949], Loss: 0.0026\n",
      "Epoch [2/10], Step[3300/10949], Loss: 0.0031\n",
      "Epoch [2/10], Step[3400/10949], Loss: 0.0020\n",
      "Epoch [2/10], Step[3500/10949], Loss: 0.0023\n",
      "Epoch [2/10], Step[3600/10949], Loss: 0.0025\n",
      "Epoch [2/10], Step[3700/10949], Loss: 0.0029\n",
      "Epoch [2/10], Step[3800/10949], Loss: 0.0025\n",
      "Epoch [2/10], Step[3900/10949], Loss: 0.0034\n",
      "Epoch [2/10], Step[4000/10949], Loss: 0.0029\n",
      "Epoch [2/10], Step[4100/10949], Loss: 0.0025\n",
      "Epoch [2/10], Step[4200/10949], Loss: 0.0027\n",
      "Epoch [2/10], Step[4300/10949], Loss: 0.0027\n",
      "Epoch [2/10], Step[4400/10949], Loss: 0.0025\n",
      "Epoch [2/10], Step[4500/10949], Loss: 0.0036\n",
      "Epoch [2/10], Step[4600/10949], Loss: 0.0024\n",
      "Epoch [2/10], Step[4700/10949], Loss: 0.0022\n",
      "Epoch [2/10], Step[4800/10949], Loss: 0.0026\n",
      "Epoch [2/10], Step[4900/10949], Loss: 0.0033\n",
      "Epoch [2/10], Step[5000/10949], Loss: 0.0020\n",
      "Epoch [2/10], Step[5100/10949], Loss: 0.0032\n",
      "Epoch [2/10], Step[5200/10949], Loss: 0.0019\n",
      "Epoch [2/10], Step[5300/10949], Loss: 0.0034\n",
      "Epoch [2/10], Step[5400/10949], Loss: 0.0027\n",
      "Epoch [2/10], Step[5500/10949], Loss: 0.0025\n",
      "Epoch [2/10], Step[5600/10949], Loss: 0.0026\n",
      "Epoch [2/10], Step[5700/10949], Loss: 0.0026\n",
      "Epoch [2/10], Step[5800/10949], Loss: 0.0038\n",
      "Epoch [2/10], Step[5900/10949], Loss: 0.0026\n",
      "Epoch [2/10], Step[6000/10949], Loss: 0.0029\n",
      "Epoch [2/10], Step[6100/10949], Loss: 0.0026\n",
      "Epoch [2/10], Step[6200/10949], Loss: 0.0024\n",
      "Epoch [2/10], Step[6300/10949], Loss: 0.0023\n",
      "Epoch [2/10], Step[6400/10949], Loss: 0.0026\n",
      "Epoch [2/10], Step[6500/10949], Loss: 0.0022\n",
      "Epoch [2/10], Step[6600/10949], Loss: 0.0027\n",
      "Epoch [2/10], Step[6700/10949], Loss: 0.0033\n",
      "Epoch [2/10], Step[6800/10949], Loss: 0.0026\n",
      "Epoch [2/10], Step[6900/10949], Loss: 0.0033\n",
      "Epoch [2/10], Step[7000/10949], Loss: 0.0023\n",
      "Epoch [2/10], Step[7100/10949], Loss: 0.0021\n",
      "Epoch [2/10], Step[7200/10949], Loss: 0.0035\n",
      "Epoch [2/10], Step[7300/10949], Loss: 0.0026\n",
      "Epoch [2/10], Step[7400/10949], Loss: 0.0034\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Step[7500/10949], Loss: 0.0028\n",
      "Epoch [2/10], Step[7600/10949], Loss: 0.0025\n",
      "Epoch [2/10], Step[7700/10949], Loss: 0.0019\n",
      "Epoch [2/10], Step[7800/10949], Loss: 0.0032\n",
      "Epoch [2/10], Step[7900/10949], Loss: 0.0027\n",
      "Epoch [2/10], Step[8000/10949], Loss: 0.0024\n",
      "Epoch [2/10], Step[8100/10949], Loss: 0.0030\n",
      "Epoch [2/10], Step[8200/10949], Loss: 0.0022\n",
      "Epoch [2/10], Step[8300/10949], Loss: 0.0025\n",
      "Epoch [2/10], Step[8400/10949], Loss: 0.0029\n",
      "Epoch [2/10], Step[8500/10949], Loss: 0.0023\n",
      "Epoch [2/10], Step[8600/10949], Loss: 0.0024\n",
      "Epoch [2/10], Step[8700/10949], Loss: 0.0026\n",
      "Epoch [2/10], Step[8800/10949], Loss: 0.0023\n",
      "Epoch [2/10], Step[8900/10949], Loss: 0.0018\n",
      "Epoch [2/10], Step[9000/10949], Loss: 0.0023\n",
      "Epoch [2/10], Step[9100/10949], Loss: 0.0025\n",
      "Epoch [2/10], Step[9200/10949], Loss: 0.0032\n",
      "Epoch [2/10], Step[9300/10949], Loss: 0.0020\n",
      "Epoch [2/10], Step[9400/10949], Loss: 0.0015\n",
      "Epoch [2/10], Step[9500/10949], Loss: 0.0028\n",
      "Epoch [2/10], Step[9600/10949], Loss: 0.0029\n",
      "Epoch [2/10], Step[9700/10949], Loss: 0.0027\n",
      "Epoch [2/10], Step[9800/10949], Loss: 0.0021\n",
      "Epoch [2/10], Step[9900/10949], Loss: 0.0026\n",
      "Epoch [2/10], Step[10000/10949], Loss: 0.0025\n",
      "Epoch [2/10], Step[10100/10949], Loss: 0.0020\n",
      "Epoch [2/10], Step[10200/10949], Loss: 0.0019\n",
      "Epoch [2/10], Step[10300/10949], Loss: 0.0027\n",
      "Epoch [2/10], Step[10400/10949], Loss: 0.0028\n",
      "Epoch [2/10], Step[10500/10949], Loss: 0.0027\n",
      "Epoch [2/10], Step[10600/10949], Loss: 0.0025\n",
      "Epoch [2/10], Step[10700/10949], Loss: 0.0027\n",
      "Epoch [2/10], Step[10800/10949], Loss: 0.0026\n",
      "Epoch [2/10], Step[10900/10949], Loss: 0.0021\n",
      "Epoch [3/10], Step[100/10949], Loss: 0.0028\n",
      "Epoch [3/10], Step[200/10949], Loss: 0.0020\n",
      "Epoch [3/10], Step[300/10949], Loss: 0.0034\n",
      "Epoch [3/10], Step[400/10949], Loss: 0.0023\n",
      "Epoch [3/10], Step[500/10949], Loss: 0.0021\n",
      "Epoch [3/10], Step[600/10949], Loss: 0.0031\n",
      "Epoch [3/10], Step[700/10949], Loss: 0.0034\n",
      "Epoch [3/10], Step[800/10949], Loss: 0.0026\n",
      "Epoch [3/10], Step[900/10949], Loss: 0.0024\n",
      "Epoch [3/10], Step[1000/10949], Loss: 0.0028\n",
      "Epoch [3/10], Step[1100/10949], Loss: 0.0030\n",
      "Epoch [3/10], Step[1200/10949], Loss: 0.0023\n",
      "Epoch [3/10], Step[1300/10949], Loss: 0.0026\n",
      "Epoch [3/10], Step[1400/10949], Loss: 0.0026\n",
      "Epoch [3/10], Step[1500/10949], Loss: 0.0026\n",
      "Epoch [3/10], Step[1600/10949], Loss: 0.0025\n",
      "Epoch [3/10], Step[1700/10949], Loss: 0.0023\n",
      "Epoch [3/10], Step[1800/10949], Loss: 0.0026\n",
      "Epoch [3/10], Step[1900/10949], Loss: 0.0023\n",
      "Epoch [3/10], Step[2000/10949], Loss: 0.0025\n",
      "Epoch [3/10], Step[2100/10949], Loss: 0.0021\n",
      "Epoch [3/10], Step[2200/10949], Loss: 0.0025\n",
      "Epoch [3/10], Step[2300/10949], Loss: 0.0021\n",
      "Epoch [3/10], Step[2400/10949], Loss: 0.0023\n",
      "Epoch [3/10], Step[2500/10949], Loss: 0.0026\n",
      "Epoch [3/10], Step[2600/10949], Loss: 0.0020\n",
      "Epoch [3/10], Step[2700/10949], Loss: 0.0027\n",
      "Epoch [3/10], Step[2800/10949], Loss: 0.0024\n",
      "Epoch [3/10], Step[2900/10949], Loss: 0.0026\n",
      "Epoch [3/10], Step[3000/10949], Loss: 0.0029\n",
      "Epoch [3/10], Step[3100/10949], Loss: 0.0033\n",
      "Epoch [3/10], Step[3200/10949], Loss: 0.0025\n",
      "Epoch [3/10], Step[3300/10949], Loss: 0.0032\n",
      "Epoch [3/10], Step[3400/10949], Loss: 0.0029\n",
      "Epoch [3/10], Step[3500/10949], Loss: 0.0030\n",
      "Epoch [3/10], Step[3600/10949], Loss: 0.0020\n",
      "Epoch [3/10], Step[3700/10949], Loss: 0.0022\n",
      "Epoch [3/10], Step[3800/10949], Loss: 0.0023\n",
      "Epoch [3/10], Step[3900/10949], Loss: 0.0022\n",
      "Epoch [3/10], Step[4000/10949], Loss: 0.0021\n",
      "Epoch [3/10], Step[4100/10949], Loss: 0.0029\n",
      "Epoch [3/10], Step[4200/10949], Loss: 0.0023\n",
      "Epoch [3/10], Step[4300/10949], Loss: 0.0018\n",
      "Epoch [3/10], Step[4400/10949], Loss: 0.0021\n",
      "Epoch [3/10], Step[4500/10949], Loss: 0.0025\n",
      "Epoch [3/10], Step[4600/10949], Loss: 0.0023\n",
      "Epoch [3/10], Step[4700/10949], Loss: 0.0027\n",
      "Epoch [3/10], Step[4800/10949], Loss: 0.0024\n",
      "Epoch [3/10], Step[4900/10949], Loss: 0.0029\n",
      "Epoch [3/10], Step[5000/10949], Loss: 0.0023\n",
      "Epoch [3/10], Step[5100/10949], Loss: 0.0020\n",
      "Epoch [3/10], Step[5200/10949], Loss: 0.0025\n",
      "Epoch [3/10], Step[5300/10949], Loss: 0.0031\n",
      "Epoch [3/10], Step[5400/10949], Loss: 0.0020\n",
      "Epoch [3/10], Step[5500/10949], Loss: 0.0030\n",
      "Epoch [3/10], Step[5600/10949], Loss: 0.0022\n",
      "Epoch [3/10], Step[5700/10949], Loss: 0.0024\n",
      "Epoch [3/10], Step[5800/10949], Loss: 0.0026\n",
      "Epoch [3/10], Step[5900/10949], Loss: 0.0025\n",
      "Epoch [3/10], Step[6000/10949], Loss: 0.0032\n",
      "Epoch [3/10], Step[6100/10949], Loss: 0.0022\n",
      "Epoch [3/10], Step[6200/10949], Loss: 0.0023\n",
      "Epoch [3/10], Step[6300/10949], Loss: 0.0030\n",
      "Epoch [3/10], Step[6400/10949], Loss: 0.0029\n",
      "Epoch [3/10], Step[6500/10949], Loss: 0.0025\n",
      "Epoch [3/10], Step[6600/10949], Loss: 0.0029\n",
      "Epoch [3/10], Step[6700/10949], Loss: 0.0029\n",
      "Epoch [3/10], Step[6800/10949], Loss: 0.0025\n",
      "Epoch [3/10], Step[6900/10949], Loss: 0.0020\n",
      "Epoch [3/10], Step[7000/10949], Loss: 0.0024\n",
      "Epoch [3/10], Step[7100/10949], Loss: 0.0022\n",
      "Epoch [3/10], Step[7200/10949], Loss: 0.0020\n",
      "Epoch [3/10], Step[7300/10949], Loss: 0.0029\n",
      "Epoch [3/10], Step[7400/10949], Loss: 0.0019\n",
      "Epoch [3/10], Step[7500/10949], Loss: 0.0026\n",
      "Epoch [3/10], Step[7600/10949], Loss: 0.0026\n",
      "Epoch [3/10], Step[7700/10949], Loss: 0.0030\n",
      "Epoch [3/10], Step[7800/10949], Loss: 0.0020\n",
      "Epoch [3/10], Step[7900/10949], Loss: 0.0023\n",
      "Epoch [3/10], Step[8000/10949], Loss: 0.0028\n",
      "Epoch [3/10], Step[8100/10949], Loss: 0.0027\n",
      "Epoch [3/10], Step[8200/10949], Loss: 0.0023\n",
      "Epoch [3/10], Step[8300/10949], Loss: 0.0026\n",
      "Epoch [3/10], Step[8400/10949], Loss: 0.0028\n",
      "Epoch [3/10], Step[8500/10949], Loss: 0.0023\n",
      "Epoch [3/10], Step[8600/10949], Loss: 0.0028\n",
      "Epoch [3/10], Step[8700/10949], Loss: 0.0026\n",
      "Epoch [3/10], Step[8800/10949], Loss: 0.0024\n",
      "Epoch [3/10], Step[8900/10949], Loss: 0.0023\n",
      "Epoch [3/10], Step[9000/10949], Loss: 0.0024\n",
      "Epoch [3/10], Step[9100/10949], Loss: 0.0029\n",
      "Epoch [3/10], Step[9200/10949], Loss: 0.0026\n",
      "Epoch [3/10], Step[9300/10949], Loss: 0.0031\n",
      "Epoch [3/10], Step[9400/10949], Loss: 0.0021\n",
      "Epoch [3/10], Step[9500/10949], Loss: 0.0031\n",
      "Epoch [3/10], Step[9600/10949], Loss: 0.0020\n",
      "Epoch [3/10], Step[9700/10949], Loss: 0.0026\n",
      "Epoch [3/10], Step[9800/10949], Loss: 0.0020\n",
      "Epoch [3/10], Step[9900/10949], Loss: 0.0026\n",
      "Epoch [3/10], Step[10000/10949], Loss: 0.0024\n",
      "Epoch [3/10], Step[10100/10949], Loss: 0.0022\n",
      "Epoch [3/10], Step[10200/10949], Loss: 0.0022\n",
      "Epoch [3/10], Step[10300/10949], Loss: 0.0027\n",
      "Epoch [3/10], Step[10400/10949], Loss: 0.0032\n",
      "Epoch [3/10], Step[10500/10949], Loss: 0.0028\n",
      "Epoch [3/10], Step[10600/10949], Loss: 0.0020\n",
      "Epoch [3/10], Step[10700/10949], Loss: 0.0023\n",
      "Epoch [3/10], Step[10800/10949], Loss: 0.0026\n",
      "Epoch [3/10], Step[10900/10949], Loss: 0.0022\n",
      "Epoch [4/10], Step[100/10949], Loss: 0.0024\n",
      "Epoch [4/10], Step[200/10949], Loss: 0.0022\n",
      "Epoch [4/10], Step[300/10949], Loss: 0.0023\n",
      "Epoch [4/10], Step[400/10949], Loss: 0.0024\n",
      "Epoch [4/10], Step[500/10949], Loss: 0.0021\n",
      "Epoch [4/10], Step[600/10949], Loss: 0.0026\n",
      "Epoch [4/10], Step[700/10949], Loss: 0.0021\n",
      "Epoch [4/10], Step[800/10949], Loss: 0.0026\n",
      "Epoch [4/10], Step[900/10949], Loss: 0.0025\n",
      "Epoch [4/10], Step[1000/10949], Loss: 0.0024\n",
      "Epoch [4/10], Step[1100/10949], Loss: 0.0027\n",
      "Epoch [4/10], Step[1200/10949], Loss: 0.0019\n",
      "Epoch [4/10], Step[1300/10949], Loss: 0.0023\n",
      "Epoch [4/10], Step[1400/10949], Loss: 0.0028\n",
      "Epoch [4/10], Step[1500/10949], Loss: 0.0027\n",
      "Epoch [4/10], Step[1600/10949], Loss: 0.0025\n",
      "Epoch [4/10], Step[1700/10949], Loss: 0.0029\n",
      "Epoch [4/10], Step[1800/10949], Loss: 0.0026\n",
      "Epoch [4/10], Step[1900/10949], Loss: 0.0030\n",
      "Epoch [4/10], Step[2000/10949], Loss: 0.0030\n",
      "Epoch [4/10], Step[2100/10949], Loss: 0.0023\n",
      "Epoch [4/10], Step[2200/10949], Loss: 0.0024\n",
      "Epoch [4/10], Step[2300/10949], Loss: 0.0024\n",
      "Epoch [4/10], Step[2400/10949], Loss: 0.0028\n",
      "Epoch [4/10], Step[2500/10949], Loss: 0.0025\n",
      "Epoch [4/10], Step[2600/10949], Loss: 0.0021\n",
      "Epoch [4/10], Step[2700/10949], Loss: 0.0018\n",
      "Epoch [4/10], Step[2800/10949], Loss: 0.0027\n",
      "Epoch [4/10], Step[2900/10949], Loss: 0.0037\n",
      "Epoch [4/10], Step[3000/10949], Loss: 0.0017\n",
      "Epoch [4/10], Step[3100/10949], Loss: 0.0028\n",
      "Epoch [4/10], Step[3200/10949], Loss: 0.0019\n",
      "Epoch [4/10], Step[3300/10949], Loss: 0.0026\n",
      "Epoch [4/10], Step[3400/10949], Loss: 0.0022\n",
      "Epoch [4/10], Step[3500/10949], Loss: 0.0023\n",
      "Epoch [4/10], Step[3600/10949], Loss: 0.0022\n",
      "Epoch [4/10], Step[3700/10949], Loss: 0.0026\n",
      "Epoch [4/10], Step[3800/10949], Loss: 0.0031\n",
      "Epoch [4/10], Step[3900/10949], Loss: 0.0022\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], Step[4000/10949], Loss: 0.0025\n",
      "Epoch [4/10], Step[4100/10949], Loss: 0.0023\n",
      "Epoch [4/10], Step[4200/10949], Loss: 0.0027\n",
      "Epoch [4/10], Step[4300/10949], Loss: 0.0028\n",
      "Epoch [4/10], Step[4400/10949], Loss: 0.0023\n",
      "Epoch [4/10], Step[4500/10949], Loss: 0.0031\n",
      "Epoch [4/10], Step[4600/10949], Loss: 0.0027\n",
      "Epoch [4/10], Step[4700/10949], Loss: 0.0023\n",
      "Epoch [4/10], Step[4800/10949], Loss: 0.0024\n",
      "Epoch [4/10], Step[4900/10949], Loss: 0.0026\n",
      "Epoch [4/10], Step[5000/10949], Loss: 0.0020\n",
      "Epoch [4/10], Step[5100/10949], Loss: 0.0023\n",
      "Epoch [4/10], Step[5200/10949], Loss: 0.0024\n",
      "Epoch [4/10], Step[5300/10949], Loss: 0.0022\n",
      "Epoch [4/10], Step[5400/10949], Loss: 0.0023\n",
      "Epoch [4/10], Step[5500/10949], Loss: 0.0020\n",
      "Epoch [4/10], Step[5600/10949], Loss: 0.0024\n",
      "Epoch [4/10], Step[5700/10949], Loss: 0.0028\n",
      "Epoch [4/10], Step[5800/10949], Loss: 0.0024\n",
      "Epoch [4/10], Step[5900/10949], Loss: 0.0024\n",
      "Epoch [4/10], Step[6000/10949], Loss: 0.0020\n",
      "Epoch [4/10], Step[6100/10949], Loss: 0.0033\n",
      "Epoch [4/10], Step[6200/10949], Loss: 0.0023\n",
      "Epoch [4/10], Step[6300/10949], Loss: 0.0023\n",
      "Epoch [4/10], Step[6400/10949], Loss: 0.0022\n",
      "Epoch [4/10], Step[6500/10949], Loss: 0.0025\n",
      "Epoch [4/10], Step[6600/10949], Loss: 0.0019\n",
      "Epoch [4/10], Step[6700/10949], Loss: 0.0026\n",
      "Epoch [4/10], Step[6800/10949], Loss: 0.0026\n",
      "Epoch [4/10], Step[6900/10949], Loss: 0.0026\n",
      "Epoch [4/10], Step[7000/10949], Loss: 0.0025\n",
      "Epoch [4/10], Step[7100/10949], Loss: 0.0026\n",
      "Epoch [4/10], Step[7200/10949], Loss: 0.0021\n",
      "Epoch [4/10], Step[7300/10949], Loss: 0.0030\n",
      "Epoch [4/10], Step[7400/10949], Loss: 0.0026\n",
      "Epoch [4/10], Step[7500/10949], Loss: 0.0025\n",
      "Epoch [4/10], Step[7600/10949], Loss: 0.0029\n",
      "Epoch [4/10], Step[7700/10949], Loss: 0.0029\n",
      "Epoch [4/10], Step[7800/10949], Loss: 0.0029\n",
      "Epoch [4/10], Step[7900/10949], Loss: 0.0023\n",
      "Epoch [4/10], Step[8000/10949], Loss: 0.0026\n",
      "Epoch [4/10], Step[8100/10949], Loss: 0.0026\n",
      "Epoch [4/10], Step[8200/10949], Loss: 0.0018\n",
      "Epoch [4/10], Step[8300/10949], Loss: 0.0028\n",
      "Epoch [4/10], Step[8400/10949], Loss: 0.0020\n",
      "Epoch [4/10], Step[8500/10949], Loss: 0.0024\n",
      "Epoch [4/10], Step[8600/10949], Loss: 0.0026\n",
      "Epoch [4/10], Step[8700/10949], Loss: 0.0023\n",
      "Epoch [4/10], Step[8800/10949], Loss: 0.0025\n",
      "Epoch [4/10], Step[8900/10949], Loss: 0.0025\n",
      "Epoch [4/10], Step[9000/10949], Loss: 0.0023\n",
      "Epoch [4/10], Step[9100/10949], Loss: 0.0026\n",
      "Epoch [4/10], Step[9200/10949], Loss: 0.0024\n",
      "Epoch [4/10], Step[9300/10949], Loss: 0.0025\n",
      "Epoch [4/10], Step[9400/10949], Loss: 0.0025\n",
      "Epoch [4/10], Step[9500/10949], Loss: 0.0027\n",
      "Epoch [4/10], Step[9600/10949], Loss: 0.0027\n",
      "Epoch [4/10], Step[9700/10949], Loss: 0.0025\n",
      "Epoch [4/10], Step[9800/10949], Loss: 0.0027\n",
      "Epoch [4/10], Step[9900/10949], Loss: 0.0020\n",
      "Epoch [4/10], Step[10000/10949], Loss: 0.0020\n",
      "Epoch [4/10], Step[10100/10949], Loss: 0.0030\n",
      "Epoch [4/10], Step[10200/10949], Loss: 0.0020\n",
      "Epoch [4/10], Step[10300/10949], Loss: 0.0030\n",
      "Epoch [4/10], Step[10400/10949], Loss: 0.0026\n",
      "Epoch [4/10], Step[10500/10949], Loss: 0.0025\n",
      "Epoch [4/10], Step[10600/10949], Loss: 0.0020\n",
      "Epoch [4/10], Step[10700/10949], Loss: 0.0028\n",
      "Epoch [4/10], Step[10800/10949], Loss: 0.0025\n",
      "Epoch [4/10], Step[10900/10949], Loss: 0.0027\n",
      "Epoch [5/10], Step[100/10949], Loss: 0.0032\n",
      "Epoch [5/10], Step[200/10949], Loss: 0.0024\n",
      "Epoch [5/10], Step[300/10949], Loss: 0.0021\n",
      "Epoch [5/10], Step[400/10949], Loss: 0.0025\n",
      "Epoch [5/10], Step[500/10949], Loss: 0.0023\n",
      "Epoch [5/10], Step[600/10949], Loss: 0.0037\n",
      "Epoch [5/10], Step[700/10949], Loss: 0.0026\n",
      "Epoch [5/10], Step[800/10949], Loss: 0.0025\n",
      "Epoch [5/10], Step[900/10949], Loss: 0.0022\n",
      "Epoch [5/10], Step[1000/10949], Loss: 0.0026\n",
      "Epoch [5/10], Step[1100/10949], Loss: 0.0028\n",
      "Epoch [5/10], Step[1200/10949], Loss: 0.0027\n",
      "Epoch [5/10], Step[1300/10949], Loss: 0.0029\n",
      "Epoch [5/10], Step[1400/10949], Loss: 0.0022\n",
      "Epoch [5/10], Step[1500/10949], Loss: 0.0025\n",
      "Epoch [5/10], Step[1600/10949], Loss: 0.0022\n",
      "Epoch [5/10], Step[1700/10949], Loss: 0.0019\n",
      "Epoch [5/10], Step[1800/10949], Loss: 0.0020\n",
      "Epoch [5/10], Step[1900/10949], Loss: 0.0027\n",
      "Epoch [5/10], Step[2000/10949], Loss: 0.0022\n",
      "Epoch [5/10], Step[2100/10949], Loss: 0.0020\n",
      "Epoch [5/10], Step[2200/10949], Loss: 0.0024\n",
      "Epoch [5/10], Step[2300/10949], Loss: 0.0041\n",
      "Epoch [5/10], Step[2400/10949], Loss: 0.0023\n",
      "Epoch [5/10], Step[2500/10949], Loss: 0.0023\n",
      "Epoch [5/10], Step[2600/10949], Loss: 0.0024\n",
      "Epoch [5/10], Step[2700/10949], Loss: 0.0026\n",
      "Epoch [5/10], Step[2800/10949], Loss: 0.0020\n",
      "Epoch [5/10], Step[2900/10949], Loss: 0.0027\n",
      "Epoch [5/10], Step[3000/10949], Loss: 0.0026\n",
      "Epoch [5/10], Step[3100/10949], Loss: 0.0028\n",
      "Epoch [5/10], Step[3200/10949], Loss: 0.0039\n",
      "Epoch [5/10], Step[3300/10949], Loss: 0.0024\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for i, data_batch in enumerate(dataloader):\n",
    "        x = Variable(data_batch['text'].float())\n",
    "        x = x.cuda()\n",
    "        y = x\n",
    "        \n",
    "        output = model(x)\n",
    "        optimizer.zero_grad()\n",
    "        loss = lossfun(output[0], y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1)%100 == 0:\n",
    "            print('Epoch [%d/%d], Step[%d/%d], Loss: %0.4f'\n",
    "                  %(epoch+1, num_epochs, i+1, len(data)//batch_size, loss.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
